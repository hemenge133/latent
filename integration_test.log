2025-03-28 00:26:31.235 | INFO     | __main__:test_checkpoint_resume_integration:152 - Starting initial training run
2025-03-28 00:26:31.235 | INFO     | __main__:run_command_with_logging:104 - Running command: python main.py --d-model 64 --num-layers 2 --num-latent 4 --min-digits 1 --max-digits 1 --batch-size 16 --max-steps 15 --save-every 5 --seed 42
2025-03-28 00:26:35.592 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/Devel/latent/src/TrainingLoop.py:356: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
2025-03-28 00:26:35.592 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/Devel/latent/src/TrainingLoop.py:375: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
2025-03-28 00:26:38.956 | WARNING  | __main__:_read_output:98 - STDERR: Training (steps):   0%|          | 0/15 [00:00<?, ?it/s]/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:26:38.956 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:26:39.044 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:26:39.044 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:26:42.591 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:26:42.592 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:26:43.421 | WARNING  | __main__:_read_output:98 - STDERR: Training (steps):   7%|â–‹         | 1/15 [00:07<01:49,  7.82s/it, simple_loss=2.1393, latent_loss=2.0289]/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:26:43.421 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:26:48.210 | WARNING  | __main__:_read_output:98 - STDERR: Training (steps):  20%|â–ˆâ–ˆ        | 3/15 [00:07<00:25,  2.09s/it, simple_loss=1.0179, latent_loss=0.9943]/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:26:48.210 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:26:54.794 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:26:54.794 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:26:55.202 | WARNING  | __main__:_read_output:98 - STDERR: Training (steps):  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:19<00:59,  5.38s/it, simple_loss=2.1377, latent_loss=2.1545]/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:26:55.202 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:26:55.422 | WARNING  | __main__:_read_output:98 - STDERR: Val SimpleTransformer:   0%|          | 0/4 [00:00<?, ?it/s][A/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)
2025-03-28 00:26:55.653 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:26:55.653 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:26:55.843 | WARNING  | __main__:_read_output:98 - STDERR: Val SimpleTransformer:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:01,  1.66it/s][A/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:26:55.844 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:26:56.173 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:26:56.173 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:26:56.522 | WARNING  | __main__:_read_output:98 - STDERR: [A/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:26:56.522 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:26:57.045 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:26:57.045 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:26:57.239 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:26:57.239 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:26:57.580 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:26:57.581 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:26:57.581 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:26:57.581 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:26:57.582 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:26:58.385 | WARNING  | __main__:_read_output:98 - STDERR: Val LatentTransformer:   0%|          | 0/4 [00:00<?, ?it/s][A/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:26:58.386 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:26:58.580 | WARNING  | __main__:_read_output:98 - STDERR: Val LatentTransformer:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:02,  1.01it/s][A/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:26:58.580 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:27:00.018 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:27:00.019 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:27:00.763 | WARNING  | __main__:_read_output:98 - STDERR: [A/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:27:00.764 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:27:01.863 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:27:01.863 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:27:02.050 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:27:02.051 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:27:02.750 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:27:02.750 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:27:02.751 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:27:02.751 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:27:02.752 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:27:03.140 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:27:03.141 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:27:03.141 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:27:03.141 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:27:03.142 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:27:03.929 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:27:03.930 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:27:03.930 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:27:03.930 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:27:03.931 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:27:04.318 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:27:04.318 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:27:04.319 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:27:04.319 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:27:04.320 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:27:04.719 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:27:04.720 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:27:04.720 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:27:04.722 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:27:04.722 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:27:07.061 | INFO     | __main__:test_checkpoint_resume_integration:180 - Initial training process exit code: 0
2025-03-28 00:27:07.061 | INFO     | __main__:test_checkpoint_resume_integration:187 - Initial training completed successfully
2025-03-28 00:27:07.061 | WARNING  | __main__:get_latest_run_id_from_runs_file:35 - runs.json file not found
2025-03-28 00:27:07.062 | INFO     | __main__:test_checkpoint_resume_integration:201 - Found run ID from directory listing: 20250328-002635_d64_l2_n4
2025-03-28 00:27:07.062 | INFO     | __main__:test_checkpoint_resume_integration:204 - Extracted run ID: 20250328-002635_d64_l2_n4
2025-03-28 00:27:07.094 | INFO     | __main__:test_checkpoint_resume_integration:217 - SimpleTransformer checkpoint created at step 15
2025-03-28 00:27:08.095 | INFO     | __main__:test_checkpoint_resume_integration:223 - Resuming training for more steps with run ID: 20250328-002635_d64_l2_n4
2025-03-28 00:27:08.096 | INFO     | __main__:run_command_with_logging:104 - Running command: python main.py --resume --run-id 20250328-002635_d64_l2_n4 --max-steps 25 --seed 42
2025-03-28 00:27:10.216 | INFO     | __main__:_read_output:100 - STDERR: 2025-03-28 00:27:10.216 | INFO     | __main__:main:400 - Using checkpoint paths from run 20250328-002635_d64_l2_n4
2025-03-28 00:27:10.217 | INFO     | __main__:_read_output:100 - STDERR: 2025-03-28 00:27:10.216 | INFO     | __main__:main:417 - Loading checkpoints to extract model dimensions
2025-03-28 00:27:10.632 | INFO     | __main__:_read_output:100 - STDERR: 2025-03-28 00:27:10.632 | INFO     | __main__:main:426 - Dimensions from SimpleTransformer checkpoint: {'d_model': 320, 'num_layers': 5, 'vocab_size': 12, 'num_latent': 10}
2025-03-28 00:27:10.633 | INFO     | __main__:_read_output:100 - STDERR: 2025-03-28 00:27:10.632 | INFO     | __main__:main:427 - Dimensions from LatentTransformer checkpoint: {'d_model': 320, 'num_layers': 5, 'vocab_size': 12, 'num_latent': 10}
2025-03-28 00:27:10.633 | INFO     | __main__:_read_output:100 - STDERR: 2025-03-28 00:27:10.633 | INFO     | __main__:main:431 - Setting d_model to 320 from checkpoint
2025-03-28 00:27:10.633 | INFO     | __main__:_read_output:100 - STDERR: 2025-03-28 00:27:10.633 | INFO     | __main__:main:435 - Setting num_layers to 5 from checkpoint
2025-03-28 00:27:10.634 | INFO     | __main__:_read_output:100 - STDERR: 2025-03-28 00:27:10.633 | INFO     | __main__:main:439 - Setting num_latent to 10 from checkpoint
2025-03-28 00:27:10.634 | INFO     | __main__:_read_output:100 - STDERR: 2025-03-28 00:27:10.633 | INFO     | __main__:main:446 - Using vocab_size=12 from checkpoint
2025-03-28 00:27:13.330 | INFO     | __main__:_read_output:100 - STDERR: 2025-03-28 00:27:13.330 | INFO     | __main__:main:681 - Loading SimpleTransformer checkpoint from checkpoints/simpletransformer/simpletransformer_latest.pt
2025-03-28 00:27:13.423 | ERROR    | __main__:_read_output:96 - STDERR: 2025-03-28 00:27:13.422 | ERROR    | __main__:main:689 - Error loading SimpleTransformer weights: Error(s) in loading state_dict for OptimizedModule:
2025-03-28 00:27:13.423 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.pos_encoder: copying a param with shape torch.Size([20, 64]) from checkpoint, the shape in current model is torch.Size([20, 320]).
2025-03-28 00:27:13.424 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.embed.weight: copying a param with shape torch.Size([12, 64]) from checkpoint, the shape in current model is torch.Size([12, 320]).
2025-03-28 00:27:13.424 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([192, 64]) from checkpoint, the shape in current model is torch.Size([960, 320]).
2025-03-28 00:27:13.424 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([960]).
2025-03-28 00:27:13.424 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([320, 320]).
2025-03-28 00:27:13.425 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:27:13.425 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.linear1.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1280, 320]).
2025-03-28 00:27:13.425 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1280]).
2025-03-28 00:27:13.426 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.linear2.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([320, 1280]).
2025-03-28 00:27:13.426 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.linear2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:27:13.426 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.norm1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:27:13.427 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.norm1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:27:13.427 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:27:13.427 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:27:13.428 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.1.self_attn.in_proj_weight: copying a param with shape torch.Size([192, 64]) from checkpoint, the shape in current model is torch.Size([960, 320]).
2025-03-28 00:27:13.428 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.1.self_attn.in_proj_bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([960]).
2025-03-28 00:27:13.428 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.1.self_attn.out_proj.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([320, 320]).
2025-03-28 00:27:13.428 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.1.self_attn.out_proj.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:27:13.428 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.1.linear1.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1280, 320]).
2025-03-28 00:27:13.429 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.1.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1280]).
2025-03-28 00:27:13.429 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.1.linear2.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([320, 1280]).
2025-03-28 00:27:13.429 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.1.linear2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:27:13.430 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.1.norm1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:27:13.430 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.1.norm1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:27:13.430 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.1.norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:27:13.431 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.1.norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:27:13.431 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([192, 64]) from checkpoint, the shape in current model is torch.Size([960, 320]).
2025-03-28 00:27:13.431 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([960]).
2025-03-28 00:27:13.432 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([320, 320]).
2025-03-28 00:27:13.432 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:27:13.432 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.multihead_attn.in_proj_weight: copying a param with shape torch.Size([192, 64]) from checkpoint, the shape in current model is torch.Size([960, 320]).
2025-03-28 00:27:13.432 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.multihead_attn.in_proj_bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([960]).
2025-03-28 00:27:13.433 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.multihead_attn.out_proj.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([320, 320]).
2025-03-28 00:27:13.433 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.multihead_attn.out_proj.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:27:13.434 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.linear1.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1280, 320]).
2025-03-28 00:27:13.434 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1280]).
2025-03-28 00:27:13.434 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.linear2.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([320, 1280]).
2025-03-28 00:27:13.435 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.linear2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:27:13.435 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.norm1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:27:13.435 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.norm1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:27:13.436 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:27:13.436 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:27:13.436 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.norm3.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:27:13.437 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.norm3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:27:13.437 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.1.self_attn.in_proj_weight: copying a param with shape torch.Size([192, 64]) from checkpoint, the shape in current model is torch.Size([960, 320]).
2025-03-28 00:27:13.437 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.1.self_attn.in_proj_bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([960]).
2025-03-28 00:27:13.438 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.1.self_attn.out_proj.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([320, 320]).
2025-03-28 00:27:13.438 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.1.self_attn.out_proj.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:27:13.438 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.1.multihead_attn.in_proj_weight: copying a param with shape torch.Size([192, 64]) from checkpoint, the shape in current model is torch.Size([960, 320]).
2025-03-28 00:27:13.439 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.1.multihead_attn.in_proj_bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([960]).
2025-03-28 00:27:13.439 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.1.multihead_attn.out_proj.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([320, 320]).
2025-03-28 00:27:13.439 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.1.multihead_attn.out_proj.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:27:13.439 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.1.linear1.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1280, 320]).
2025-03-28 00:27:13.440 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.1.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1280]).
2025-03-28 00:27:13.440 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.1.linear2.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([320, 1280]).
2025-03-28 00:27:13.440 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.1.linear2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:27:13.440 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.1.norm1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:27:13.441 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.1.norm1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:27:13.441 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.1.norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:27:13.441 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.1.norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:27:13.441 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.1.norm3.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:27:13.442 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.1.norm3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:27:13.442 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.output_proj.weight: copying a param with shape torch.Size([12, 64]) from checkpoint, the shape in current model is torch.Size([12, 320]).
2025-03-28 00:27:13.442 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:27:13.442 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:27:13.443 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:27:13.443 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:27:13.443 | ERROR    | __main__:_read_output:96 - STDERR: 2025-03-28 00:27:13.423 | ERROR    | __main__:main:692 - This might be due to dimension mismatch. Check model configuration.
2025-03-28 00:27:15.237 | INFO     | __main__:test_checkpoint_resume_integration:239 - Resume training process exit code: 1
2025-03-28 00:27:15.237 | ERROR    | __main__:test_checkpoint_resume_integration:243 - Resume training failed. Log output: 2025-03-28 00:27:10.213 | INFO     | __main__:main:294 - Using device: cuda
2025-03-28 00:27:10.213 | INFO     | __main__:main:298 - Using accuracy-aware loss with weight: 0.5
2025-03-28 00:27:10.214 | INFO     | __main__:main:299 - This reduces loss for correct token predictions during training
2025-03-28 00:27:10.214 | INFO     | __main__:main:321 - Attempting to resume from run ID: 20250328-002635_d64_l2_n4
2025-03-28 00:27:10.214 | INFO     | __main__:main:325 - Found run: 20250328-002635_d64_l2_n4
2025-03-28 00:27:10.215 | INFO     | __main__:main:337 - Using seed=42 from run config (override 42)
2025-03-28 00:27:10.215 | INFO     | __main__:main:346 - Using d_model=64 from run config (override 384)
2025-03-28 00:27:10.215 | INFO     | __main__:main:352 - Using num_layers=2 from run config (override 4)
2025-03-28 00:27:10.216 | INFO     | __main__:main:358 - Using num_latent=4 from run config (override 8)
2025-03-28 00:27:10.216 | INFO     | __main__:main:400 - Using checkpoint paths from run 20250328-002635_d64_l2_n4
2025-03-28 00:27:10.216 | INFO     | __main__:main:417 - Loading checkpoints to extract model dimensions
2025-03-28 00:27:10.632 | INFO     | __main__:main:426 - Dimensions from SimpleTransformer checkpoint: {'d_model': 320, 'num_layers': 5, 'vocab_size': 12, 'num_latent': 10}
2025-03-28 00:27:10.632 | INFO     | __main__:main:427 - Dimensions from LatentTransformer checkpoint: {'d_model': 320, 'num_layers': 5, 'vocab_size': 12, 'num_latent': 10}
2025-03-28 00:27:10.633 | INFO     | __main__:main:431 - Setting d_model to 320 from checkpoint
2025-03-28 00:27:10.633 | INFO     | __main__:main:435 - Setting num_layers to 5 from checkpoint
2025-03-28 00:27:10.633 | INFO     | __main__:main:439 - Setting num_latent to 10 from checkpoint
2025-03-28 00:27:10.633 | INFO     | __main__:main:446 - Using vocab_size=12 from checkpoint
2025-03-28 00:27:10.633 | INFO     | __main__:main:469 - Using train dataset with range 1-99
2025-03-28 00:27:10.646 | INFO     | __main__:main:478 - Using val dataset with range 1-99
2025-03-28 00:27:10.647 | INFO     | __main__:main:535 - Using vocabulary size: 12
2025-03-28 00:27:13.249 | INFO     | __main__:main:589 - Models compiled with torch.compile()
2025-03-28 00:27:13.326 | INFO     | __main__:main:601 -
Model Parameters:
2025-03-28 00:27:13.326 | INFO     | __main__:main:602 - SimpleTransformer: 14,402,572
2025-03-28 00:27:13.326 | INFO     | __main__:main:603 - LatentTransformer: 15,536,012
2025-03-28 00:27:13.326 | INFO     | __main__:main:604 - Parameter ratio: 1.08x
2025-03-28 00:27:13.327 | INFO     | __main__:main:605 - Difference: 1,133,440 parameters (7.9%)
2025-03-28 00:27:13.328 | INFO     | __main__:main:637 -
Training both models in parallel...
2025-03-28 00:27:13.328 | INFO     | __main__:main:643 - Using consistent log directory for resumed training: runs/parallel_comparison/20250328-002635_d64_l2_n4
2025-03-28 00:27:13.330 | INFO     | __main__:main:681 - Loading SimpleTransformer checkpoint from checkpoints/simpletransformer/simpletransformer_latest.pt
2025-03-28 00:27:13.422 | ERROR    | __main__:main:689 - Error loading SimpleTransformer weights: Error(s) in loading state_dict for OptimizedModule:
size mismatch for _orig_mod.pos_encoder: copying a param with shape torch.Size([20, 64]) from checkpoint, the shape in current model is torch.Size([20, 320]).
Using train dataset with range 1-99: 7840 problems
Using val dataset with range 1-99: 980 problems
size mismatch for _orig_mod.embed.weight: copying a param with shape torch.Size([12, 64]) from checkpoint, the shape in current model is torch.Size([12, 320]).
size mismatch for _orig_mod.encoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([192, 64]) from checkpoint, the shape in current model is torch.Size([960, 320]).
size mismatch for _orig_mod.encoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([960]).
size mismatch for _orig_mod.encoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([320, 320]).
size mismatch for _orig_mod.encoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
size mismatch for _orig_mod.encoder.layers.0.linear1.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1280, 320]).
size mismatch for _orig_mod.encoder.layers.0.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1280]).
size mismatch for _orig_mod.encoder.layers.0.linear2.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([320, 1280]).
size mismatch for _orig_mod.encoder.layers.0.linear2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
size mismatch for _orig_mod.encoder.layers.0.norm1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
size mismatch for _orig_mod.encoder.layers.0.norm1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
size mismatch for _orig_mod.encoder.layers.0.norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
size mismatch for _orig_mod.encoder.layers.0.norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
size mismatch for _orig_mod.encoder.layers.1.self_attn.in_proj_weight: copying a param with shape torch.Size([192, 64]) from checkpoint, the shape in current model is torch.Size([960, 320]).
size mismatch for _orig_mod.encoder.layers.1.self_attn.in_proj_bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([960]).
size mismatch for _orig_mod.encoder.layers.1.self_attn.out_proj.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([320, 320]).
size mismatch for _orig_mod.encoder.layers.1.self_attn.out_proj.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
size mismatch for _orig_mod.encoder.layers.1.linear1.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1280, 320]).
size mismatch for _orig_mod.encoder.layers.1.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1280]).
size mismatch for _orig_mod.encoder.layers.1.linear2.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([320, 1280]).
size mismatch for _orig_mod.encoder.layers.1.linear2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
size mismatch for _orig_mod.encoder.layers.1.norm1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
size mismatch for _orig_mod.encoder.layers.1.norm1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
size mismatch for _orig_mod.encoder.layers.1.norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
size mismatch for _orig_mod.encoder.layers.1.norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
size mismatch for _orig_mod.decoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([192, 64]) from checkpoint, the shape in current model is torch.Size([960, 320]).
size mismatch for _orig_mod.decoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([960]).
size mismatch for _orig_mod.decoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([320, 320]).
size mismatch for _orig_mod.decoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
size mismatch for _orig_mod.decoder.layers.0.multihead_attn.in_proj_weight: copying a param with shape torch.Size([192, 64]) from checkpoint, the shape in current model is torch.Size([960, 320]).
size mismatch for _orig_mod.decoder.layers.0.multihead_attn.in_proj_bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([960]).
size mismatch for _orig_mod.decoder.layers.0.multihead_attn.out_proj.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([320, 320]).
size mismatch for _orig_mod.decoder.layers.0.multihead_attn.out_proj.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
size mismatch for _orig_mod.decoder.layers.0.linear1.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1280, 320]).
size mismatch for _orig_mod.decoder.layers.0.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1280]).
size mismatch for _orig_mod.decoder.layers.0.linear2.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([320, 1280]).
size mismatch for _orig_mod.decoder.layers.0.linear2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
size mismatch for _orig_mod.decoder.layers.0.norm1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
size mismatch for _orig_mod.decoder.layers.0.norm1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
size mismatch for _orig_mod.decoder.layers.0.norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
size mismatch for _orig_mod.decoder.layers.0.norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
size mismatch for _orig_mod.decoder.layers.0.norm3.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
size mismatch for _orig_mod.decoder.layers.0.norm3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
size mismatch for _orig_mod.decoder.layers.1.self_attn.in_proj_weight: copying a param with shape torch.Size([192, 64]) from checkpoint, the shape in current model is torch.Size([960, 320]).
size mismatch for _orig_mod.decoder.layers.1.self_attn.in_proj_bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([960]).
size mismatch for _orig_mod.decoder.layers.1.self_attn.out_proj.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([320, 320]).
size mismatch for _orig_mod.decoder.layers.1.self_attn.out_proj.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
size mismatch for _orig_mod.decoder.layers.1.multihead_attn.in_proj_weight: copying a param with shape torch.Size([192, 64]) from checkpoint, the shape in current model is torch.Size([960, 320]).
size mismatch for _orig_mod.decoder.layers.1.multihead_attn.in_proj_bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([960]).
size mismatch for _orig_mod.decoder.layers.1.multihead_attn.out_proj.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([320, 320]).
size mismatch for _orig_mod.decoder.layers.1.multihead_attn.out_proj.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
size mismatch for _orig_mod.decoder.layers.1.linear1.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1280, 320]).
size mismatch for _orig_mod.decoder.layers.1.linear1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1280]).
size mismatch for _orig_mod.decoder.layers.1.linear2.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([320, 1280]).
size mismatch for _orig_mod.decoder.layers.1.linear2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
size mismatch for _orig_mod.decoder.layers.1.norm1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
size mismatch for _orig_mod.decoder.layers.1.norm1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
size mismatch for _orig_mod.decoder.layers.1.norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
size mismatch for _orig_mod.decoder.layers.1.norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
size mismatch for _orig_mod.decoder.layers.1.norm3.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
size mismatch for _orig_mod.decoder.layers.1.norm3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
size mismatch for _orig_mod.output_proj.weight: copying a param with shape torch.Size([12, 64]) from checkpoint, the shape in current model is torch.Size([12, 320]).
size mismatch for _orig_mod.encoder_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
size mismatch for _orig_mod.encoder_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
size mismatch for _orig_mod.decoder_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
size mismatch for _orig_mod.decoder_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:27:13.423 | ERROR    | __main__:main:692 - This might be due to dimension mismatch. Check model configuration.

2025-03-28 00:27:15.238 | ERROR    | __main__:test_checkpoint_resume_integration:278 - Test failed with exception: Resume training failed with code 1
Traceback (most recent call last):

  File "/home/h/Devel/latent/tests/integration/test_checkpoint_resume.py", line 294, in <module>
    test_checkpoint_resume_integration()
    â”” <function test_checkpoint_resume_integration at 0x7f1a69e227a0>

> File "/home/h/Devel/latent/tests/integration/test_checkpoint_resume.py", line 244, in test_checkpoint_resume_integration
    assert False, f"Resume training failed with code {resume_exit_code}"

AssertionError: Resume training failed with code 1
2025-03-28 00:27:15.239 | INFO     | __main__:test_checkpoint_resume_integration:284 - Cleaning up test run: 20250328-002635_d64_l2_n4
2025-03-28 00:27:15.240 | INFO     | __main__:cleanup_test_run:67 - Removing run directory: runs/parallel_comparison/20250328-002635_d64_l2_n4
2025-03-28 00:27:15.240 | INFO     | __main__:test_checkpoint_resume_integration:290 - Removing log file: initial_run.log
2025-03-28 00:27:15.240 | INFO     | __main__:test_checkpoint_resume_integration:290 - Removing log file: resume_run.log
2025-03-28 00:29:50.053 | INFO     | __main__:test_checkpoint_resume_integration:152 - Starting initial training run
2025-03-28 00:29:50.054 | INFO     | __main__:run_command_with_logging:104 - Running command: python main.py --d-model 64 --num-layers 2 --num-latent 4 --min-digits 1 --max-digits 1 --batch-size 16 --max-steps 15 --save-every 5 --seed 42
2025-03-28 00:29:54.425 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/Devel/latent/src/TrainingLoop.py:356: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
2025-03-28 00:29:54.425 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/Devel/latent/src/TrainingLoop.py:375: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
2025-03-28 00:29:57.970 | WARNING  | __main__:_read_output:98 - STDERR: Training (steps):   0%|          | 0/15 [00:00<?, ?it/s]/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:29:57.971 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:29:58.066 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:29:58.066 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:30:01.661 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:30:01.661 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:30:02.463 | WARNING  | __main__:_read_output:98 - STDERR: Training (steps):   7%|â–‹         | 1/15 [00:08<01:52,  8.03s/it, simple_loss=2.1393, latent_loss=2.0289]/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:30:02.464 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:30:06.695 | INFO     | __main__:test_checkpoint_resume_integration:290 - Removing log file: initial_run.log
2025-03-28 00:30:38.576 | INFO     | __main__:test_checkpoint_resume_integration:152 - Starting initial training run
2025-03-28 00:30:38.576 | INFO     | __main__:run_command_with_logging:104 - Running command: python main.py --d-model 32 --num-layers 1 --num-latent 2 --min-digits 1 --max-digits 1 --batch-size 8 --max-steps 10 --save-every 5 --seed 42
2025-03-28 00:30:43.012 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/Devel/latent/src/TrainingLoop.py:356: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
2025-03-28 00:30:43.012 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/Devel/latent/src/TrainingLoop.py:375: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
2025-03-28 00:30:47.761 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:30:47.761 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:30:56.338 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:30:56.339 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:30:59.414 | WARNING  | __main__:_read_output:98 - STDERR: Training (steps):  10%|â–ˆ         | 1/10 [00:16<02:27, 16.39s/it, simple_loss=1.6329, latent_loss=1.5669]/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:30:59.415 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:30:59.926 | WARNING  | __main__:_read_output:98 - STDERR: Val SimpleTransformer:   0%|          | 0/8 [00:00<?, ?it/s][A/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)
2025-03-28 00:31:00.353 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:31:00.354 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:31:00.656 | WARNING  | __main__:_read_output:98 - STDERR: Val SimpleTransformer:  12%|â–ˆâ–Ž        | 1/8 [00:01<00:07,  1.01s/it][A/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:31:00.658 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:31:01.718 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:31:01.718 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:31:02.512 | WARNING  | __main__:_read_output:98 - STDERR: [A/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:31:02.513 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:31:03.591 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:31:03.592 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:31:03.923 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:31:03.923 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:31:04.185 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:31:04.186 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:31:04.186 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:31:04.186 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:31:04.187 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:31:06.173 | WARNING  | __main__:_read_output:98 - STDERR: Val LatentTransformer:   0%|          | 0/8 [00:00<?, ?it/s][A/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:31:06.174 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:31:06.292 | WARNING  | __main__:_read_output:98 - STDERR: Val LatentTransformer:  12%|â–ˆâ–Ž        | 1/8 [00:02<00:14,  2.10s/it][A/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:31:06.293 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:31:09.428 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:31:09.428 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:31:12.016 | WARNING  | __main__:_read_output:98 - STDERR: [A/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:31:12.017 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:31:13.319 | WARNING  | __main__:_read_output:98 - STDERR: Training (steps):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:30<00:25,  4.30s/it, simple_loss=2.0769, latent_loss=2.1916]/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:31:13.319 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:31:13.518 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:31:13.518 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:31:25.118 | WARNING  | __main__:_read_output:98 - STDERR: Training (steps):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:30<00:10,  3.33s/it, simple_loss=1.8021, latent_loss=1.7534]/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:31:25.119 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:31:39.497 | WARNING  | __main__:_read_output:98 - STDERR: Training (steps):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:50<00:10,  3.33s/it, simple_loss=1.8021, latent_loss=1.7534]/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:31:39.498 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:31:39.887 | WARNING  | __main__:_read_output:98 - STDERR: Training (steps):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:56<00:17,  8.60s/it, simple_loss=1.1025, latent_loss=1.1294]/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:31:39.887 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:31:39.974 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:31:39.974 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:31:39.974 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:31:39.975 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:31:40.284 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:31:40.284 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:31:40.285 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:31:40.285 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:31:40.286 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:31:40.640 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:31:40.641 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:31:40.641 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:31:40.642 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:31:43.898 | INFO     | __main__:test_checkpoint_resume_integration:180 - Initial training process exit code: 0
2025-03-28 00:31:43.899 | INFO     | __main__:test_checkpoint_resume_integration:187 - Initial training completed successfully
2025-03-28 00:31:43.899 | WARNING  | __main__:get_latest_run_id_from_runs_file:35 - runs.json file not found
2025-03-28 00:31:43.899 | INFO     | __main__:test_checkpoint_resume_integration:201 - Found run ID from directory listing: 20250328-003043_d32_l1_n2
2025-03-28 00:31:43.899 | INFO     | __main__:test_checkpoint_resume_integration:204 - Extracted run ID: 20250328-003043_d32_l1_n2
2025-03-28 00:31:43.917 | INFO     | __main__:test_checkpoint_resume_integration:217 - SimpleTransformer checkpoint created at step 10
2025-03-28 00:31:44.919 | INFO     | __main__:test_checkpoint_resume_integration:223 - Resuming training for more steps with run ID: 20250328-003043_d32_l1_n2
2025-03-28 00:31:44.919 | INFO     | __main__:run_command_with_logging:104 - Running command: python main.py --resume --run-id 20250328-003043_d32_l1_n2 --max-steps 20 --seed 42
2025-03-28 00:31:47.009 | INFO     | __main__:_read_output:100 - STDERR: 2025-03-28 00:31:47.009 | INFO     | __main__:main:413 - Using checkpoint paths from run 20250328-003043_d32_l1_n2
2025-03-28 00:31:47.009 | INFO     | __main__:_read_output:100 - STDERR: 2025-03-28 00:31:47.009 | INFO     | __main__:main:430 - Loading checkpoints to extract model dimensions
2025-03-28 00:31:47.367 | INFO     | __main__:_read_output:100 - STDERR: 2025-03-28 00:31:47.366 | INFO     | __main__:main:439 - Dimensions from SimpleTransformer checkpoint: {'d_model': 320, 'num_layers': 5, 'vocab_size': 12, 'num_latent': 10, 'max_len': 20}
2025-03-28 00:31:47.367 | INFO     | __main__:_read_output:100 - STDERR: 2025-03-28 00:31:47.367 | INFO     | __main__:main:440 - Dimensions from LatentTransformer checkpoint: {'d_model': 320, 'num_layers': 5, 'vocab_size': 12, 'num_latent': 10, 'max_len': 20}
2025-03-28 00:31:47.368 | INFO     | __main__:_read_output:100 - STDERR: 2025-03-28 00:31:47.367 | INFO     | __main__:main:444 - Setting d_model to 320 from checkpoint
2025-03-28 00:31:47.368 | INFO     | __main__:_read_output:100 - STDERR: 2025-03-28 00:31:47.368 | INFO     | __main__:main:448 - Setting num_layers to 5 from checkpoint
2025-03-28 00:31:47.369 | INFO     | __main__:_read_output:100 - STDERR: 2025-03-28 00:31:47.368 | INFO     | __main__:main:452 - Setting num_latent to 10 from checkpoint
2025-03-28 00:31:47.369 | INFO     | __main__:_read_output:100 - STDERR: 2025-03-28 00:31:47.368 | INFO     | __main__:main:459 - Using vocab_size=12 from checkpoint
2025-03-28 00:31:49.950 | INFO     | __main__:_read_output:100 - STDERR: 2025-03-28 00:31:49.950 | INFO     | __main__:main:694 - Loading SimpleTransformer checkpoint from checkpoints/simpletransformer/simpletransformer_latest.pt
2025-03-28 00:31:49.993 | ERROR    | __main__:_read_output:96 - STDERR: 2025-03-28 00:31:49.992 | ERROR    | __main__:main:702 - Error loading SimpleTransformer weights: Error(s) in loading state_dict for OptimizedModule:
2025-03-28 00:31:49.993 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.pos_encoder: copying a param with shape torch.Size([20, 32]) from checkpoint, the shape in current model is torch.Size([20, 320]).
2025-03-28 00:31:49.993 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.embed.weight: copying a param with shape torch.Size([12, 32]) from checkpoint, the shape in current model is torch.Size([12, 320]).
2025-03-28 00:31:49.994 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([96, 32]) from checkpoint, the shape in current model is torch.Size([960, 320]).
2025-03-28 00:31:49.994 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([960]).
2025-03-28 00:31:49.994 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([32, 32]) from checkpoint, the shape in current model is torch.Size([320, 320]).
2025-03-28 00:31:49.994 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:49.995 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.linear1.weight: copying a param with shape torch.Size([128, 32]) from checkpoint, the shape in current model is torch.Size([1280, 320]).
2025-03-28 00:31:49.995 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.linear1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1280]).
2025-03-28 00:31:49.995 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.linear2.weight: copying a param with shape torch.Size([32, 128]) from checkpoint, the shape in current model is torch.Size([320, 1280]).
2025-03-28 00:31:49.995 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.linear2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:49.996 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.norm1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:49.996 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.norm1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:49.996 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.norm2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:49.996 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.norm2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:49.997 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([96, 32]) from checkpoint, the shape in current model is torch.Size([960, 320]).
2025-03-28 00:31:49.997 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([960]).
2025-03-28 00:31:49.997 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([32, 32]) from checkpoint, the shape in current model is torch.Size([320, 320]).
2025-03-28 00:31:49.997 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:49.998 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.multihead_attn.in_proj_weight: copying a param with shape torch.Size([96, 32]) from checkpoint, the shape in current model is torch.Size([960, 320]).
2025-03-28 00:31:49.998 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.multihead_attn.in_proj_bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([960]).
2025-03-28 00:31:49.998 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.multihead_attn.out_proj.weight: copying a param with shape torch.Size([32, 32]) from checkpoint, the shape in current model is torch.Size([320, 320]).
2025-03-28 00:31:49.998 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.multihead_attn.out_proj.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:49.999 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.linear1.weight: copying a param with shape torch.Size([128, 32]) from checkpoint, the shape in current model is torch.Size([1280, 320]).
2025-03-28 00:31:49.999 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.linear1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1280]).
2025-03-28 00:31:49.999 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.linear2.weight: copying a param with shape torch.Size([32, 128]) from checkpoint, the shape in current model is torch.Size([320, 1280]).
2025-03-28 00:31:49.999 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.linear2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:50.000 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.norm1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:50.000 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.norm1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:50.000 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.norm2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:50.001 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.norm2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:50.001 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.norm3.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:50.001 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.norm3.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:50.001 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.output_proj.weight: copying a param with shape torch.Size([12, 32]) from checkpoint, the shape in current model is torch.Size([12, 320]).
2025-03-28 00:31:50.002 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder_norm.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:50.002 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder_norm.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:50.002 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder_norm.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:50.002 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder_norm.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:50.003 | ERROR    | __main__:_read_output:96 - STDERR: 2025-03-28 00:31:49.993 | ERROR    | __main__:main:705 - This might be due to dimension mismatch. Check model configuration.
2025-03-28 00:31:50.003 | INFO     | __main__:_read_output:100 - STDERR: 2025-03-28 00:31:49.993 | INFO     | __main__:main:744 - Loading LatentTransformer checkpoint from checkpoints/latenttransformer/latenttransformer_latest.pt
2025-03-28 00:31:50.056 | ERROR    | __main__:_read_output:96 - STDERR: 2025-03-28 00:31:50.056 | ERROR    | __main__:main:752 - Error loading LatentTransformer weights: Error(s) in loading state_dict for OptimizedModule:
2025-03-28 00:31:50.056 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.pos_encoder: copying a param with shape torch.Size([20, 32]) from checkpoint, the shape in current model is torch.Size([20, 320]).
2025-03-28 00:31:50.057 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_tokens: copying a param with shape torch.Size([2, 32]) from checkpoint, the shape in current model is torch.Size([10, 320]).
2025-03-28 00:31:50.057 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.embed.weight: copying a param with shape torch.Size([12, 32]) from checkpoint, the shape in current model is torch.Size([12, 320]).
2025-03-28 00:31:50.057 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([96, 32]) from checkpoint, the shape in current model is torch.Size([960, 320]).
2025-03-28 00:31:50.058 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([960]).
2025-03-28 00:31:50.058 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([32, 32]) from checkpoint, the shape in current model is torch.Size([320, 320]).
2025-03-28 00:31:50.058 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:50.058 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.linear1.weight: copying a param with shape torch.Size([128, 32]) from checkpoint, the shape in current model is torch.Size([1280, 320]).
2025-03-28 00:31:50.059 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.linear1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1280]).
2025-03-28 00:31:50.059 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.linear2.weight: copying a param with shape torch.Size([32, 128]) from checkpoint, the shape in current model is torch.Size([320, 1280]).
2025-03-28 00:31:50.059 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.linear2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:50.060 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.norm1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:50.060 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.norm1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:50.060 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.norm2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:50.061 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.norm2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:50.061 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_proj_q.weight: copying a param with shape torch.Size([32, 32]) from checkpoint, the shape in current model is torch.Size([320, 320]).
2025-03-28 00:31:50.061 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_proj_q.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:50.062 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_proj_k.weight: copying a param with shape torch.Size([32, 32]) from checkpoint, the shape in current model is torch.Size([320, 320]).
2025-03-28 00:31:50.062 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_proj_k.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:50.062 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_proj_v.weight: copying a param with shape torch.Size([32, 32]) from checkpoint, the shape in current model is torch.Size([320, 320]).
2025-03-28 00:31:50.063 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_proj_v.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:50.063 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_norm1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:50.063 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_norm1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:50.064 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_norm2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:50.064 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_norm2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:50.065 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_mlp.0.weight: copying a param with shape torch.Size([128, 32]) from checkpoint, the shape in current model is torch.Size([1280, 320]).
2025-03-28 00:31:50.065 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_mlp.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1280]).
2025-03-28 00:31:50.065 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_mlp.2.weight: copying a param with shape torch.Size([32, 128]) from checkpoint, the shape in current model is torch.Size([320, 1280]).
2025-03-28 00:31:50.066 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_mlp.2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:50.066 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([96, 32]) from checkpoint, the shape in current model is torch.Size([960, 320]).
2025-03-28 00:31:50.066 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([960]).
2025-03-28 00:31:50.067 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([32, 32]) from checkpoint, the shape in current model is torch.Size([320, 320]).
2025-03-28 00:31:50.067 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:50.068 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.multihead_attn.in_proj_weight: copying a param with shape torch.Size([96, 32]) from checkpoint, the shape in current model is torch.Size([960, 320]).
2025-03-28 00:31:50.068 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.multihead_attn.in_proj_bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([960]).
2025-03-28 00:31:50.069 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.multihead_attn.out_proj.weight: copying a param with shape torch.Size([32, 32]) from checkpoint, the shape in current model is torch.Size([320, 320]).
2025-03-28 00:31:50.069 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.multihead_attn.out_proj.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:50.070 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.linear1.weight: copying a param with shape torch.Size([128, 32]) from checkpoint, the shape in current model is torch.Size([1280, 320]).
2025-03-28 00:31:50.070 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.linear1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1280]).
2025-03-28 00:31:50.070 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.linear2.weight: copying a param with shape torch.Size([32, 128]) from checkpoint, the shape in current model is torch.Size([320, 1280]).
2025-03-28 00:31:50.071 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.linear2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:50.071 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.norm1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:50.071 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.norm1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:50.072 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.norm2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:50.072 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.norm2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:50.073 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.norm3.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:50.073 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.norm3.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:50.073 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.output_proj.weight: copying a param with shape torch.Size([12, 32]) from checkpoint, the shape in current model is torch.Size([12, 320]).
2025-03-28 00:31:50.074 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder_norm.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:50.074 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder_norm.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:50.075 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder_norm.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:50.075 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder_norm.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:31:50.075 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/Devel/latent/src/TrainingLoop.py:356: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
2025-03-28 00:31:50.076 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/Devel/latent/src/TrainingLoop.py:375: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
2025-03-28 00:31:50.076 | ERROR    | __main__:_read_output:96 - STDERR: Error loading SimpleTransformer checkpoint: loaded state dict contains a parameter group that doesn't match the size of optimizer's group
2025-03-28 00:31:50.076 | INFO     | __main__:_read_output:100 - STDERR: simple_optimizer.load_state_dict(simple_checkpoint['optimizer_state_dict'])
2025-03-28 00:31:50.076 | ERROR    | __main__:_read_output:96 - STDERR: raise ValueError(
2025-03-28 00:31:50.077 | ERROR    | __main__:_read_output:96 - STDERR: ValueError: loaded state dict contains a parameter group that doesn't match the size of optimizer's group
2025-03-28 00:31:50.077 | ERROR    | __main__:_read_output:96 - STDERR: Error loading LatentTransformer checkpoint: loaded state dict contains a parameter group that doesn't match the size of optimizer's group
2025-03-28 00:31:50.077 | INFO     | __main__:_read_output:100 - STDERR: latent_optimizer.load_state_dict(latent_checkpoint['optimizer_state_dict'])
2025-03-28 00:31:50.077 | ERROR    | __main__:_read_output:96 - STDERR: raise ValueError(
2025-03-28 00:31:50.078 | ERROR    | __main__:_read_output:96 - STDERR: ValueError: loaded state dict contains a parameter group that doesn't match the size of optimizer's group
2025-03-28 00:31:59.849 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:31:59.850 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:31:59.946 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:31:59.946 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:32:17.193 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:32:17.193 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:32:23.278 | WARNING  | __main__:_read_output:98 - STDERR: Training (steps):  10%|â–ˆ         | 1/10 [00:33<04:58, 33.19s/it, simple_loss=2.7523, latent_loss=2.7372]/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:32:23.279 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:32:26.101 | WARNING  | __main__:_read_output:98 - STDERR: Final SimpleTransformer validation:   0%|          | 0/4 [00:00<?, ?it/s][A/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:32:26.101 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:32:26.428 | WARNING  | __main__:_read_output:98 - STDERR: Final SimpleTransformer validation:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:01<00:03,  1.13s/it][A/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:32:26.429 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:32:27.616 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:32:27.617 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:32:28.380 | WARNING  | __main__:_read_output:98 - STDERR: [A/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)
2025-03-28 00:32:28.868 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:32:28.868 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:32:30.073 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:32:30.074 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:32:30.754 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:32:30.754 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:32:33.277 | WARNING  | __main__:_read_output:98 - STDERR: Final LatentTransformer validation:   0%|          | 0/4 [00:00<?, ?it/s][A/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:32:33.278 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:32:33.425 | WARNING  | __main__:_read_output:98 - STDERR: Final LatentTransformer validation:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:02<00:06,  2.20s/it][A/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:32:33.426 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:32:36.513 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:32:36.513 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:32:39.630 | WARNING  | __main__:_read_output:98 - STDERR: [A/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:32:39.631 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:32:41.348 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:32:41.348 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:32:41.559 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:32:41.559 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:32:44.881 | INFO     | __main__:test_checkpoint_resume_integration:239 - Resume training process exit code: 0
2025-03-28 00:32:44.881 | INFO     | __main__:test_checkpoint_resume_integration:246 - Resume training completed successfully
2025-03-28 00:32:44.882 | ERROR    | __main__:test_checkpoint_resume_integration:262 - Error detected in resumed training: 'dimension mismatch'
2025-03-28 00:32:44.883 | ERROR    | __main__:test_checkpoint_resume_integration:263 - Context: is torch.Size([320]).
2025-03-28 00:31:49.993 | ERROR    | __main__:main:705 - This might be due to dimension mismatch. Check model configuration.
2025-03-28 00:31:49.993 | INFO     | __main__:main:74
2025-03-28 00:32:44.883 | ERROR    | __main__:test_checkpoint_resume_integration:278 - Test failed with exception: Error detected in resumed training: 'dimension mismatch'
Traceback (most recent call last):

  File "/home/h/Devel/latent/tests/integration/test_checkpoint_resume.py", line 294, in <module>
    test_checkpoint_resume_integration()
    â”” <function test_checkpoint_resume_integration at 0x7f1c0bf5a7a0>

> File "/home/h/Devel/latent/tests/integration/test_checkpoint_resume.py", line 264, in test_checkpoint_resume_integration
    assert False, f"Error detected in resumed training: '{error}'"

AssertionError: Error detected in resumed training: 'dimension mismatch'
2025-03-28 00:32:44.885 | INFO     | __main__:test_checkpoint_resume_integration:284 - Cleaning up test run: 20250328-003043_d32_l1_n2
2025-03-28 00:32:44.885 | INFO     | __main__:cleanup_test_run:67 - Removing run directory: runs/parallel_comparison/20250328-003043_d32_l1_n2
2025-03-28 00:32:44.886 | INFO     | __main__:test_checkpoint_resume_integration:290 - Removing log file: initial_run.log
2025-03-28 00:32:44.886 | INFO     | __main__:test_checkpoint_resume_integration:290 - Removing log file: resume_run.log
2025-03-28 00:33:16.324 | INFO     | __main__:test_checkpoint_resume_integration:152 - Starting initial training run
2025-03-28 00:33:16.324 | INFO     | __main__:run_command_with_logging:104 - Running command: python main.py --d-model 32 --num-layers 1 --num-latent 2 --min-digits 1 --max-digits 1 --batch-size 8 --max-steps 10 --save-every 5 --seed 42
2025-03-28 00:33:20.640 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/Devel/latent/src/TrainingLoop.py:356: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
2025-03-28 00:33:20.641 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/Devel/latent/src/TrainingLoop.py:375: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
2025-03-28 00:33:23.432 | WARNING  | __main__:_read_output:98 - STDERR: Training (steps):   0%|          | 0/10 [00:00<?, ?it/s]/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:33:23.433 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:33:26.624 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:33:26.626 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:33:27.263 | WARNING  | __main__:_read_output:98 - STDERR: Training (steps):  10%|â–ˆ         | 1/10 [00:06<00:59,  6.61s/it, simple_loss=1.6329, latent_loss=1.5669]/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:33:27.264 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:33:27.620 | WARNING  | __main__:_read_output:98 - STDERR: Val SimpleTransformer:   0%|          | 0/8 [00:00<?, ?it/s][A/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)
2025-03-28 00:33:27.821 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:33:27.821 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:33:27.958 | WARNING  | __main__:_read_output:98 - STDERR: Val SimpleTransformer:  12%|â–ˆâ–Ž        | 1/8 [00:00<00:03,  2.09it/s][A/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:33:27.959 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:33:28.544 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:33:28.545 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:33:29.065 | WARNING  | __main__:_read_output:98 - STDERR: [A/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:33:29.065 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:33:29.589 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:33:29.589 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:33:29.950 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:33:29.951 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:33:30.170 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:33:30.171 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:33:30.171 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:33:30.172 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:33:30.172 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:33:30.881 | WARNING  | __main__:_read_output:98 - STDERR: Val LatentTransformer:   0%|          | 0/8 [00:00<?, ?it/s][A/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:33:30.881 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:33:30.998 | WARNING  | __main__:_read_output:98 - STDERR: Val LatentTransformer:  12%|â–ˆâ–Ž        | 1/8 [00:00<00:05,  1.22it/s][A/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:33:30.998 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:33:32.605 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:33:32.606 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:33:33.514 | WARNING  | __main__:_read_output:98 - STDERR: [A/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:33:33.514 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:33:34.326 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:33:34.327 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:33:34.519 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:33:34.520 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:33:38.317 | WARNING  | __main__:_read_output:98 - STDERR: Training (steps):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:14<00:04,  1.61s/it, simple_loss=1.8021, latent_loss=1.7534]/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:33:38.318 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:33:43.809 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:33:43.810 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:33:44.190 | WARNING  | __main__:_read_output:98 - STDERR: Training (steps):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:23<00:06,  3.40s/it, simple_loss=1.1025, latent_loss=1.1294]/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:33:44.190 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:33:44.276 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:33:44.276 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:33:44.276 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:33:44.277 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:33:44.595 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:33:44.595 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:33:44.595 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:33:44.596 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:33:44.596 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:33:44.912 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:33:44.912 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:33:44.913 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:33:44.914 | ERROR    | __main__:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:33:47.220 | INFO     | __main__:test_checkpoint_resume_integration:180 - Initial training process exit code: 0
2025-03-28 00:33:47.221 | INFO     | __main__:test_checkpoint_resume_integration:187 - Initial training completed successfully
2025-03-28 00:33:47.221 | WARNING  | __main__:get_latest_run_id_from_runs_file:35 - runs.json file not found
2025-03-28 00:33:47.221 | INFO     | __main__:test_checkpoint_resume_integration:201 - Found run ID from directory listing: 20250328-003320_d32_l1_n2
2025-03-28 00:33:47.221 | INFO     | __main__:test_checkpoint_resume_integration:204 - Extracted run ID: 20250328-003320_d32_l1_n2
2025-03-28 00:33:47.240 | INFO     | __main__:test_checkpoint_resume_integration:217 - SimpleTransformer checkpoint created at step 10
2025-03-28 00:33:48.242 | INFO     | __main__:test_checkpoint_resume_integration:223 - Resuming training for more steps with run ID: 20250328-003320_d32_l1_n2
2025-03-28 00:33:48.242 | INFO     | __main__:run_command_with_logging:104 - Running command: python main.py --resume --run-id 20250328-003320_d32_l1_n2 --max-steps 20 --seed 42
2025-03-28 00:33:50.308 | INFO     | __main__:_read_output:100 - STDERR: 2025-03-28 00:33:50.308 | INFO     | __main__:main:413 - Using checkpoint paths from run 20250328-003320_d32_l1_n2
2025-03-28 00:33:50.308 | INFO     | __main__:_read_output:100 - STDERR: 2025-03-28 00:33:50.308 | INFO     | __main__:main:430 - Loading checkpoints to extract model dimensions
2025-03-28 00:33:50.648 | INFO     | __main__:_read_output:100 - STDERR: 2025-03-28 00:33:50.647 | INFO     | __main__:main:439 - Dimensions from SimpleTransformer checkpoint: {'d_model': 320, 'num_layers': 5, 'vocab_size': 12, 'num_latent': 10, 'max_len': 20}
2025-03-28 00:33:50.648 | INFO     | __main__:_read_output:100 - STDERR: 2025-03-28 00:33:50.648 | INFO     | __main__:main:440 - Dimensions from LatentTransformer checkpoint: {'d_model': 320, 'num_layers': 5, 'vocab_size': 12, 'num_latent': 10, 'max_len': 20}
2025-03-28 00:33:50.648 | INFO     | __main__:_read_output:100 - STDERR: 2025-03-28 00:33:50.648 | INFO     | __main__:main:444 - Setting d_model to 320 from checkpoint
2025-03-28 00:33:50.649 | INFO     | __main__:_read_output:100 - STDERR: 2025-03-28 00:33:50.648 | INFO     | __main__:main:448 - Setting num_layers to 5 from checkpoint
2025-03-28 00:33:50.649 | INFO     | __main__:_read_output:100 - STDERR: 2025-03-28 00:33:50.649 | INFO     | __main__:main:452 - Setting num_latent to 10 from checkpoint
2025-03-28 00:33:50.650 | INFO     | __main__:_read_output:100 - STDERR: 2025-03-28 00:33:50.649 | INFO     | __main__:main:459 - Using vocab_size=12 from checkpoint
2025-03-28 00:33:53.159 | INFO     | __main__:_read_output:100 - STDERR: 2025-03-28 00:33:53.159 | INFO     | __main__:main:694 - Loading SimpleTransformer checkpoint from checkpoints/simpletransformer/simpletransformer_latest.pt
2025-03-28 00:33:53.206 | ERROR    | __main__:_read_output:96 - STDERR: 2025-03-28 00:33:53.206 | ERROR    | __main__:main:702 - Error loading SimpleTransformer weights: Error(s) in loading state_dict for OptimizedModule:
2025-03-28 00:33:53.206 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.pos_encoder: copying a param with shape torch.Size([20, 32]) from checkpoint, the shape in current model is torch.Size([20, 320]).
2025-03-28 00:33:53.207 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.embed.weight: copying a param with shape torch.Size([12, 32]) from checkpoint, the shape in current model is torch.Size([12, 320]).
2025-03-28 00:33:53.207 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([96, 32]) from checkpoint, the shape in current model is torch.Size([960, 320]).
2025-03-28 00:33:53.207 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([960]).
2025-03-28 00:33:53.207 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([32, 32]) from checkpoint, the shape in current model is torch.Size([320, 320]).
2025-03-28 00:33:53.208 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.208 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.linear1.weight: copying a param with shape torch.Size([128, 32]) from checkpoint, the shape in current model is torch.Size([1280, 320]).
2025-03-28 00:33:53.208 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.linear1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1280]).
2025-03-28 00:33:53.209 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.linear2.weight: copying a param with shape torch.Size([32, 128]) from checkpoint, the shape in current model is torch.Size([320, 1280]).
2025-03-28 00:33:53.209 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.linear2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.209 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.norm1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.209 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.norm1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.209 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.norm2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.210 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.norm2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.210 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([96, 32]) from checkpoint, the shape in current model is torch.Size([960, 320]).
2025-03-28 00:33:53.210 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([960]).
2025-03-28 00:33:53.210 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([32, 32]) from checkpoint, the shape in current model is torch.Size([320, 320]).
2025-03-28 00:33:53.211 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.211 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.multihead_attn.in_proj_weight: copying a param with shape torch.Size([96, 32]) from checkpoint, the shape in current model is torch.Size([960, 320]).
2025-03-28 00:33:53.211 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.multihead_attn.in_proj_bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([960]).
2025-03-28 00:33:53.211 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.multihead_attn.out_proj.weight: copying a param with shape torch.Size([32, 32]) from checkpoint, the shape in current model is torch.Size([320, 320]).
2025-03-28 00:33:53.212 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.multihead_attn.out_proj.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.212 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.linear1.weight: copying a param with shape torch.Size([128, 32]) from checkpoint, the shape in current model is torch.Size([1280, 320]).
2025-03-28 00:33:53.212 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.linear1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1280]).
2025-03-28 00:33:53.213 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.linear2.weight: copying a param with shape torch.Size([32, 128]) from checkpoint, the shape in current model is torch.Size([320, 1280]).
2025-03-28 00:33:53.213 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.linear2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.213 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.norm1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.214 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.norm1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.214 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.norm2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.214 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.norm2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.215 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.norm3.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.215 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.norm3.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.215 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.output_proj.weight: copying a param with shape torch.Size([12, 32]) from checkpoint, the shape in current model is torch.Size([12, 320]).
2025-03-28 00:33:53.215 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder_norm.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.216 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder_norm.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.216 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder_norm.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.216 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder_norm.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.217 | ERROR    | __main__:_read_output:96 - STDERR: 2025-03-28 00:33:53.206 | ERROR    | __main__:main:705 - This might be due to dimension mismatch. Check model configuration.
2025-03-28 00:33:53.217 | INFO     | __main__:_read_output:100 - STDERR: 2025-03-28 00:33:53.206 | INFO     | __main__:main:744 - Loading LatentTransformer checkpoint from checkpoints/latenttransformer/latenttransformer_latest.pt
2025-03-28 00:33:53.267 | ERROR    | __main__:_read_output:96 - STDERR: 2025-03-28 00:33:53.266 | ERROR    | __main__:main:752 - Error loading LatentTransformer weights: Error(s) in loading state_dict for OptimizedModule:
2025-03-28 00:33:53.267 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.pos_encoder: copying a param with shape torch.Size([20, 32]) from checkpoint, the shape in current model is torch.Size([20, 320]).
2025-03-28 00:33:53.268 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_tokens: copying a param with shape torch.Size([2, 32]) from checkpoint, the shape in current model is torch.Size([10, 320]).
2025-03-28 00:33:53.268 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.embed.weight: copying a param with shape torch.Size([12, 32]) from checkpoint, the shape in current model is torch.Size([12, 320]).
2025-03-28 00:33:53.268 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([96, 32]) from checkpoint, the shape in current model is torch.Size([960, 320]).
2025-03-28 00:33:53.269 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([960]).
2025-03-28 00:33:53.269 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([32, 32]) from checkpoint, the shape in current model is torch.Size([320, 320]).
2025-03-28 00:33:53.269 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.269 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.linear1.weight: copying a param with shape torch.Size([128, 32]) from checkpoint, the shape in current model is torch.Size([1280, 320]).
2025-03-28 00:33:53.270 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.linear1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1280]).
2025-03-28 00:33:53.270 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.linear2.weight: copying a param with shape torch.Size([32, 128]) from checkpoint, the shape in current model is torch.Size([320, 1280]).
2025-03-28 00:33:53.270 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.linear2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.270 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.norm1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.271 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.norm1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.271 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.norm2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.271 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.norm2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.271 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_proj_q.weight: copying a param with shape torch.Size([32, 32]) from checkpoint, the shape in current model is torch.Size([320, 320]).
2025-03-28 00:33:53.272 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_proj_q.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.272 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_proj_k.weight: copying a param with shape torch.Size([32, 32]) from checkpoint, the shape in current model is torch.Size([320, 320]).
2025-03-28 00:33:53.272 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_proj_k.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.273 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_proj_v.weight: copying a param with shape torch.Size([32, 32]) from checkpoint, the shape in current model is torch.Size([320, 320]).
2025-03-28 00:33:53.273 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_proj_v.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.273 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_norm1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.274 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_norm1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.274 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_norm2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.274 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_norm2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.275 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_mlp.0.weight: copying a param with shape torch.Size([128, 32]) from checkpoint, the shape in current model is torch.Size([1280, 320]).
2025-03-28 00:33:53.275 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_mlp.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1280]).
2025-03-28 00:33:53.275 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_mlp.2.weight: copying a param with shape torch.Size([32, 128]) from checkpoint, the shape in current model is torch.Size([320, 1280]).
2025-03-28 00:33:53.276 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_mlp.2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.276 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([96, 32]) from checkpoint, the shape in current model is torch.Size([960, 320]).
2025-03-28 00:33:53.276 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([960]).
2025-03-28 00:33:53.276 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([32, 32]) from checkpoint, the shape in current model is torch.Size([320, 320]).
2025-03-28 00:33:53.277 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.277 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.multihead_attn.in_proj_weight: copying a param with shape torch.Size([96, 32]) from checkpoint, the shape in current model is torch.Size([960, 320]).
2025-03-28 00:33:53.277 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.multihead_attn.in_proj_bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([960]).
2025-03-28 00:33:53.278 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.multihead_attn.out_proj.weight: copying a param with shape torch.Size([32, 32]) from checkpoint, the shape in current model is torch.Size([320, 320]).
2025-03-28 00:33:53.278 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.multihead_attn.out_proj.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.278 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.linear1.weight: copying a param with shape torch.Size([128, 32]) from checkpoint, the shape in current model is torch.Size([1280, 320]).
2025-03-28 00:33:53.279 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.linear1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1280]).
2025-03-28 00:33:53.279 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.linear2.weight: copying a param with shape torch.Size([32, 128]) from checkpoint, the shape in current model is torch.Size([320, 1280]).
2025-03-28 00:33:53.279 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.linear2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.279 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.norm1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.280 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.norm1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.280 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.norm2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.281 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.norm2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.281 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.norm3.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.281 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.norm3.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.282 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.output_proj.weight: copying a param with shape torch.Size([12, 32]) from checkpoint, the shape in current model is torch.Size([12, 320]).
2025-03-28 00:33:53.282 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder_norm.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.282 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder_norm.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.282 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder_norm.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.283 | INFO     | __main__:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder_norm.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:33:53.283 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/Devel/latent/src/TrainingLoop.py:356: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
2025-03-28 00:33:53.283 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/Devel/latent/src/TrainingLoop.py:375: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
2025-03-28 00:33:53.284 | ERROR    | __main__:_read_output:96 - STDERR: Error loading SimpleTransformer checkpoint: loaded state dict contains a parameter group that doesn't match the size of optimizer's group
2025-03-28 00:33:53.284 | INFO     | __main__:_read_output:100 - STDERR: simple_optimizer.load_state_dict(simple_checkpoint['optimizer_state_dict'])
2025-03-28 00:33:53.285 | ERROR    | __main__:_read_output:96 - STDERR: raise ValueError(
2025-03-28 00:33:53.285 | ERROR    | __main__:_read_output:96 - STDERR: ValueError: loaded state dict contains a parameter group that doesn't match the size of optimizer's group
2025-03-28 00:33:53.285 | ERROR    | __main__:_read_output:96 - STDERR: Error loading LatentTransformer checkpoint: loaded state dict contains a parameter group that doesn't match the size of optimizer's group
2025-03-28 00:33:53.285 | INFO     | __main__:_read_output:100 - STDERR: latent_optimizer.load_state_dict(latent_checkpoint['optimizer_state_dict'])
2025-03-28 00:33:53.286 | ERROR    | __main__:_read_output:96 - STDERR: raise ValueError(
2025-03-28 00:33:53.286 | ERROR    | __main__:_read_output:96 - STDERR: ValueError: loaded state dict contains a parameter group that doesn't match the size of optimizer's group
2025-03-28 00:33:58.941 | WARNING  | __main__:_read_output:98 - STDERR: Training (steps):   0%|          | 0/10 [00:00<?, ?it/s]/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:33:58.941 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:33:59.030 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:33:59.031 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:34:05.511 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:34:05.511 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:34:06.400 | WARNING  | __main__:_read_output:98 - STDERR: Training (steps):  10%|â–ˆ         | 1/10 [00:13<01:57, 13.10s/it, simple_loss=2.7523, latent_loss=2.7372]/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:34:06.400 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:34:08.724 | WARNING  | __main__:_read_output:98 - STDERR: Final SimpleTransformer validation:   0%|          | 0/4 [00:00<?, ?it/s][A/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:34:08.725 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:34:08.881 | WARNING  | __main__:_read_output:98 - STDERR: Final SimpleTransformer validation:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:01,  1.90it/s][A/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:34:08.882 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:34:09.516 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:34:09.516 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:34:09.919 | WARNING  | __main__:_read_output:98 - STDERR: [A/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)
2025-03-28 00:34:10.217 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:34:10.217 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:34:10.809 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:34:10.809 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:34:11.017 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:34:11.017 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:34:12.216 | WARNING  | __main__:_read_output:98 - STDERR: Final LatentTransformer validation:   0%|          | 0/4 [00:00<?, ?it/s][A/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:34:12.217 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:34:12.363 | WARNING  | __main__:_read_output:98 - STDERR: Final LatentTransformer validation:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:02,  1.14it/s][A/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:34:12.363 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:34:14.107 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:34:14.108 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:34:15.369 | WARNING  | __main__:_read_output:98 - STDERR: [A/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:34:15.369 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:34:16.480 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:34:16.481 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:34:16.703 | WARNING  | __main__:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:34:16.703 | WARNING  | __main__:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:34:19.061 | INFO     | __main__:test_checkpoint_resume_integration:239 - Resume training process exit code: 0
2025-03-28 00:34:19.061 | INFO     | __main__:test_checkpoint_resume_integration:246 - Resume training completed successfully
2025-03-28 00:34:19.062 | ERROR    | __main__:test_checkpoint_resume_integration:262 - Error detected in resumed training: 'dimension mismatch'
2025-03-28 00:34:19.063 | ERROR    | __main__:test_checkpoint_resume_integration:263 - Context: is torch.Size([320]).
2025-03-28 00:33:53.206 | ERROR    | __main__:main:705 - This might be due to dimension mismatch. Check model configuration.
2025-03-28 00:33:53.206 | INFO     | __main__:main:74
2025-03-28 00:34:19.063 | ERROR    | __main__:test_checkpoint_resume_integration:278 - Test failed with exception: Error detected in resumed training: 'dimension mismatch'
Traceback (most recent call last):

  File "/home/h/Devel/latent/tests/integration/test_checkpoint_resume.py", line 294, in <module>
    test_checkpoint_resume_integration()
    â”” <function test_checkpoint_resume_integration at 0x7f70451127a0>

> File "/home/h/Devel/latent/tests/integration/test_checkpoint_resume.py", line 264, in test_checkpoint_resume_integration
    assert False, f"Error detected in resumed training: '{error}'"

AssertionError: Error detected in resumed training: 'dimension mismatch'
2025-03-28 00:34:19.064 | INFO     | __main__:test_checkpoint_resume_integration:284 - Cleaning up test run: 20250328-003320_d32_l1_n2
2025-03-28 00:34:19.064 | INFO     | __main__:cleanup_test_run:67 - Removing run directory: runs/parallel_comparison/20250328-003320_d32_l1_n2
2025-03-28 00:34:19.065 | INFO     | __main__:test_checkpoint_resume_integration:290 - Removing log file: initial_run.log
2025-03-28 00:34:19.065 | INFO     | __main__:test_checkpoint_resume_integration:290 - Removing log file: resume_run.log
2025-03-28 00:38:26.670 | INFO     | test_checkpoint_loading:test_checkpoint_loading_with_layer_mismatch:64 - Loading checkpoint from /home/h/Devel/latent/checkpoints/simpletransformer/latest.pt
2025-03-28 00:38:26.936 | INFO     | test_checkpoint_loading:test_checkpoint_loading_with_layer_mismatch:83 - Detected 1 layers in the checkpoint
2025-03-28 00:38:26.936 | INFO     | test_checkpoint_loading:test_checkpoint_loading_with_layer_mismatch:103 - Creating SimpleTransformer with 1 layers (d_model=320)
2025-03-28 00:38:26.977 | INFO     | test_checkpoint_loading:test_checkpoint_loading_with_layer_mismatch:114 - Trying with filter_state_dict_for_model and strict=False
2025-03-28 00:38:26.977 | INFO     | test_checkpoint_loading:test_checkpoint_loading_with_layer_mismatch:117 - Filtered state dict has 0 keys
2025-03-28 00:38:26.978 | INFO     | test_checkpoint_loading:test_checkpoint_loading_with_layer_mismatch:120 - Loading with filtered state dict and strict=False succeeded
2025-03-28 00:38:26.980 | INFO     | test_checkpoint_loading:test_checkpoint_dimensions_extraction:138 - Loading checkpoint from /home/h/Devel/latent/checkpoints/simpletransformer/latest.pt
2025-03-28 00:38:27.019 | INFO     | test_checkpoint_loading:test_checkpoint_dimensions_extraction:143 - Checkpoint has 'model_state_dict' key
2025-03-28 00:38:27.019 | INFO     | test_checkpoint_loading:test_checkpoint_dimensions_extraction:145 - Some keys in model_state_dict: ['_orig_mod.pos_encoder', '_orig_mod.embed.weight', '_orig_mod.encoder.layers.0.self_attn.in_proj_weight', '_orig_mod.encoder.layers.0.self_attn.in_proj_bias', '_orig_mod.encoder.layers.0.self_attn.out_proj.weight']
2025-03-28 00:38:27.020 | INFO     | test_checkpoint_loading:test_checkpoint_dimensions_extraction:153 - Extracted dimensions from checkpoint: {'d_model': 320, 'num_layers': 5, 'vocab_size': 12, 'num_latent': 10, 'max_len': 20}
2025-03-28 00:38:27.020 | INFO     | test_checkpoint_loading:test_checkpoint_dimensions_extraction:159 - Creating model with intentionally different dimensions: d_model=128, num_layers=3
2025-03-28 00:38:27.042 | INFO     | test_checkpoint_loading:test_checkpoint_dimensions_extraction:180 - Loading with wrong dimensions failed as expected: Error(s) in loading state_dict for StableSimpleTransformer:
	Missing key(s) in state_dict: "pos_encoder", "embed.weight", "encoder.layers.0.self_attn.in_proj_weight", "encoder.layers.0.self_attn.in_proj_bias", "encoder.layers.0.self_attn.out_proj.weight", "encoder.layers.0.self_attn.out_proj.bias", "encoder.layers.0.linear1.weight", "encoder.layers.0.linear1.bias", "encoder.layers.0.linear2.weight", "encoder.layers.0.linear2.bias", "encoder.layers.0.norm1.weight", "encoder.layers.0.norm1.bias", "encoder.layers.0.norm2.weight", "encoder.layers.0.norm2.bias", "encoder.layers.1.self_attn.in_proj_weight", "encoder.layers.1.self_attn.in_proj_bias", "encoder.layers.1.self_attn.out_proj.weight", "encoder.layers.1.self_attn.out_proj.bias", "encoder.layers.1.linear1.weight", "encoder.layers.1.linear1.bias", "encoder.layers.1.linear2.weight", "encoder.layers.1.linear2.bias", "encoder.layers.1.norm1.weight", "encoder.layers.1.norm1.bias", "encoder.layers.1.norm2.weight", "encoder.layers.1.norm2.bias", "encoder.layers.2.self_attn.in_proj_weight", "encoder.layers.2.self_attn.in_proj_bias", "encoder.layers.2.self_attn.out_proj.weight", "encoder.layers.2.self_attn.out_proj.bias", "encoder.layers.2.linear1.weight", "encoder.layers.2.linear1.bias", "encoder.layers.2.linear2.weight", "encoder.layers.2.linear2.bias", "encoder.layers.2.norm1.weight", "encoder.layers.2.norm1.bias", "encoder.layers.2.norm2.weight", "encoder.layers.2.norm2.bias", "decoder.layers.0.self_attn.in_proj_weight", "decoder.layers.0.self_attn.in_proj_bias", "decoder.layers.0.self_attn.out_proj.weight", "decoder.layers.0.self_attn.out_proj.bias", "decoder.layers.0.multihead_attn.in_proj_weight", "decoder.layers.0.multihead_attn.in_proj_bias", "decoder.layers.0.multihead_attn.out_proj.weight", "decoder.layers.0.multihead_attn.out_proj.bias", "decoder.layers.0.linear1.weight", "decoder.layers.0.linear1.bias", "decoder.layers.0.linear2.weight", "decoder.layers.0.linear2.bias", "decoder.layers.0.norm1.weight", "decoder.layers.0.norm1.bias", "decoder.layers.0.norm2.weight", "decoder.layers.0.norm2.bias", "decoder.layers.0.norm3.weight", "decoder.layers.0.norm3.bias", "decoder.layers.1.self_attn.in_proj_weight", "decoder.layers.1.self_attn.in_proj_bias", "decoder.layers.1.self_attn.out_proj.weight", "decoder.layers.1.self_attn.out_proj.bias", "decoder.layers.1.multihead_attn.in_proj_weight", "decoder.layers.1.multihead_attn.in_proj_bias", "decoder.layers.1.multihead_attn.out_proj.weight", "decoder.layers.1.multihead_attn.out_proj.bias", "decoder.layers.1.linear1.weight", "decoder.layers.1.linear1.bias", "decoder.layers.1.linear2.weight", "decoder.layers.1.linear2.bias", "decoder.layers.1.norm1.weight", "decoder.layers.1.norm1.bias", "decoder.layers.1.norm2.weight", "decoder.layers.1.norm2.bias", "decoder.layers.1.norm3.weight", "decoder.layers.1.norm3.bias", "decoder.layers.2.self_attn.in_proj_weight", "decoder.layers.2.self_attn.in_proj_bias", "decoder.layers.2.self_attn.out_proj.weight", "decoder.layers.2.self_attn.out_proj.bias", "decoder.layers.2.multihead_attn.in_proj_weight", "decoder.layers.2.multihead_attn.in_proj_bias", "decoder.layers.2.multihead_attn.out_proj.weight", "decoder.layers.2.multihead_attn.out_proj.bias", "decoder.layers.2.linear1.weight", "decoder.layers.2.linear1.bias", "decoder.layers.2.linear2.weight", "decoder.layers.2.linear2.bias", "decoder.layers.2.norm1.weight", "decoder.layers.2.norm1.bias", "decoder.layers.2.norm2.weight", "decoder.layers.2.norm2.bias", "decoder.layers.2.norm3.weight", "decoder.layers.2.norm3.bias", "output_proj.weight", "output_proj.bias", "encoder_norm.weight", "encoder_norm.bias", "decoder_norm.weight", "decoder_norm.bias". 
	Unexpected key(s) in state_dict: "_orig_mod.pos_encoder", "_orig_mod.embed.weight", "_orig_mod.encoder.layers.0.self_attn.in_proj_weight", "_orig_mod.encoder.layers.0.self_attn.in_proj_bias", "_orig_mod.encoder.layers.0.self_attn.out_proj.weight", "_orig_mod.encoder.layers.0.self_attn.out_proj.bias", "_orig_mod.encoder.layers.0.linear1.weight", "_orig_mod.encoder.layers.0.linear1.bias", "_orig_mod.encoder.layers.0.linear2.weight", "_orig_mod.encoder.layers.0.linear2.bias", "_orig_mod.encoder.layers.0.norm1.weight", "_orig_mod.encoder.layers.0.norm1.bias", "_orig_mod.encoder.layers.0.norm2.weight", "_orig_mod.encoder.layers.0.norm2.bias", "_orig_mod.decoder.layers.0.self_attn.in_proj_weight", "_orig_mod.decoder.layers.0.self_attn.in_proj_bias", "_orig_mod.decoder.layers.0.self_attn.out_proj.weight", "_orig_mod.decoder.layers.0.self_attn.out_proj.bias", "_orig_mod.decoder.layers.0.multihead_attn.in_proj_weight", "_orig_mod.decoder.layers.0.multihead_attn.in_proj_bias", "_orig_mod.decoder.layers.0.multihead_attn.out_proj.weight", "_orig_mod.decoder.layers.0.multihead_attn.out_proj.bias", "_orig_mod.decoder.layers.0.linear1.weight", "_orig_mod.decoder.layers.0.linear1.bias", "_orig_mod.decoder.layers.0.linear2.weight", "_orig_mod.decoder.layers.0.linear2.bias", "_orig_mod.decoder.layers.0.norm1.weight", "_orig_mod.decoder.layers.0.norm1.bias", "_orig_mod.decoder.layers.0.norm2.weight", "_orig_mod.decoder.layers.0.norm2.bias", "_orig_mod.decoder.layers.0.norm3.weight", "_orig_mod.decoder.layers.0.norm3.bias", "_orig_mod.output_proj.weight", "_orig_mod.output_proj.bias", "_orig_mod.encoder_norm.weight", "_orig_mod.encoder_norm.bias", "_orig_mod.decoder_norm.weight", "_orig_mod.decoder_norm.bias". 
2025-03-28 00:38:27.042 | INFO     | test_checkpoint_loading:test_checkpoint_dimensions_extraction:184 - Recreating model with correct dimensions from checkpoint
2025-03-28 00:38:27.153 | INFO     | test_checkpoint_loading:test_checkpoint_dimensions_extraction:195 - Loading with correct dimensions succeeded
2025-03-28 00:38:27.160 | INFO     | test_checkpoint_loading:test_main_extract_model_dimensions:218 - Loading checkpoint from /home/h/Devel/latent/checkpoints/simpletransformer/latest.pt
2025-03-28 00:38:27.198 | INFO     | test_checkpoint_loading:test_main_extract_model_dimensions:225 - Extracted dimensions: {'d_model': 320, 'num_layers': 5, 'vocab_size': 12, 'num_latent': 10, 'max_len': 20}
2025-03-28 00:38:27.337 | INFO     | test_checkpoint_resume_unit:test_checkpoint_optimizer_persistence:172 - Starting first training run (15 steps)
2025-03-28 00:38:30.281 | INFO     | test_checkpoint_resume_unit:test_checkpoint_optimizer_persistence:193 - Looking for checkpoint at: /home/h/Devel/latent/checkpoints/simpletransformer/simpletransformer_latest.pt
2025-03-28 00:38:30.371 | INFO     | test_checkpoint_resume_unit:test_checkpoint_optimizer_persistence:212 - Last step from checkpoint - Simple: 10
2025-03-28 00:38:30.372 | INFO     | test_checkpoint_resume_unit:test_checkpoint_optimizer_persistence:213 - Last step from checkpoint - Latent: 10
2025-03-28 00:38:30.372 | INFO     | test_checkpoint_resume_unit:test_checkpoint_optimizer_persistence:214 - Simple optimizer info: {'lr': 1.4999999999999999e-05, 'weight_decay': 0.04, 'state_keys': 38, 'exp_avg_mean': 2.155610673071351e-05, 'exp_avg_sq_mean': 7.266904589187106e-08, 'step': 10}
2025-03-28 00:38:30.372 | INFO     | test_checkpoint_resume_unit:test_checkpoint_optimizer_persistence:215 - Simple scheduler info: {'last_epoch': 10, 'step_count': 11, 'base_lrs': [0.0003], 'verbose': False}
2025-03-28 00:38:30.372 | INFO     | test_checkpoint_resume_unit:test_checkpoint_optimizer_persistence:221 - Last LR from checkpoint - Simple: 1.4999999999999999e-05
2025-03-28 00:38:30.372 | INFO     | test_checkpoint_resume_unit:test_checkpoint_optimizer_persistence:222 - Last LR from checkpoint - Latent: 1.4999999999999999e-05
2025-03-28 00:38:30.373 | INFO     | test_checkpoint_resume_unit:test_checkpoint_optimizer_persistence:225 - Starting second training run (resumed, +15 steps)
2025-03-28 00:40:06.401 | INFO     | test_checkpoint_loading:test_checkpoint_loading_with_layer_mismatch:64 - Loading checkpoint from /home/h/Devel/latent/checkpoints/simpletransformer/latest.pt
2025-03-28 00:40:06.731 | INFO     | test_checkpoint_loading:test_checkpoint_loading_with_layer_mismatch:83 - Detected 1 layers in the checkpoint
2025-03-28 00:40:06.732 | INFO     | test_checkpoint_loading:test_checkpoint_loading_with_layer_mismatch:103 - Creating SimpleTransformer with 1 layers (d_model=320)
2025-03-28 00:40:06.785 | INFO     | test_checkpoint_loading:test_checkpoint_loading_with_layer_mismatch:114 - Trying with filter_state_dict_for_model and strict=False
2025-03-28 00:40:06.786 | INFO     | test_checkpoint_loading:test_checkpoint_loading_with_layer_mismatch:117 - Filtered state dict has 0 keys
2025-03-28 00:40:06.787 | INFO     | test_checkpoint_loading:test_checkpoint_loading_with_layer_mismatch:120 - Loading with filtered state dict and strict=False succeeded
2025-03-28 00:40:06.791 | INFO     | test_checkpoint_loading:test_checkpoint_dimensions_extraction:138 - Loading checkpoint from /home/h/Devel/latent/checkpoints/simpletransformer/latest.pt
2025-03-28 00:40:06.841 | INFO     | test_checkpoint_loading:test_checkpoint_dimensions_extraction:143 - Checkpoint has 'model_state_dict' key
2025-03-28 00:40:06.842 | INFO     | test_checkpoint_loading:test_checkpoint_dimensions_extraction:145 - Some keys in model_state_dict: ['_orig_mod.pos_encoder', '_orig_mod.embed.weight', '_orig_mod.encoder.layers.0.self_attn.in_proj_weight', '_orig_mod.encoder.layers.0.self_attn.in_proj_bias', '_orig_mod.encoder.layers.0.self_attn.out_proj.weight']
2025-03-28 00:40:06.842 | INFO     | test_checkpoint_loading:test_checkpoint_dimensions_extraction:153 - Extracted dimensions from checkpoint: {'d_model': 320, 'num_layers': 5, 'vocab_size': 12, 'num_latent': 10, 'max_len': 20}
2025-03-28 00:40:06.842 | INFO     | test_checkpoint_loading:test_checkpoint_dimensions_extraction:159 - Creating model with intentionally different dimensions: d_model=128, num_layers=3
2025-03-28 00:40:06.865 | INFO     | test_checkpoint_loading:test_checkpoint_dimensions_extraction:180 - Loading with wrong dimensions failed as expected: Error(s) in loading state_dict for StableSimpleTransformer:
	Missing key(s) in state_dict: "pos_encoder", "embed.weight", "encoder.layers.0.self_attn.in_proj_weight", "encoder.layers.0.self_attn.in_proj_bias", "encoder.layers.0.self_attn.out_proj.weight", "encoder.layers.0.self_attn.out_proj.bias", "encoder.layers.0.linear1.weight", "encoder.layers.0.linear1.bias", "encoder.layers.0.linear2.weight", "encoder.layers.0.linear2.bias", "encoder.layers.0.norm1.weight", "encoder.layers.0.norm1.bias", "encoder.layers.0.norm2.weight", "encoder.layers.0.norm2.bias", "encoder.layers.1.self_attn.in_proj_weight", "encoder.layers.1.self_attn.in_proj_bias", "encoder.layers.1.self_attn.out_proj.weight", "encoder.layers.1.self_attn.out_proj.bias", "encoder.layers.1.linear1.weight", "encoder.layers.1.linear1.bias", "encoder.layers.1.linear2.weight", "encoder.layers.1.linear2.bias", "encoder.layers.1.norm1.weight", "encoder.layers.1.norm1.bias", "encoder.layers.1.norm2.weight", "encoder.layers.1.norm2.bias", "encoder.layers.2.self_attn.in_proj_weight", "encoder.layers.2.self_attn.in_proj_bias", "encoder.layers.2.self_attn.out_proj.weight", "encoder.layers.2.self_attn.out_proj.bias", "encoder.layers.2.linear1.weight", "encoder.layers.2.linear1.bias", "encoder.layers.2.linear2.weight", "encoder.layers.2.linear2.bias", "encoder.layers.2.norm1.weight", "encoder.layers.2.norm1.bias", "encoder.layers.2.norm2.weight", "encoder.layers.2.norm2.bias", "decoder.layers.0.self_attn.in_proj_weight", "decoder.layers.0.self_attn.in_proj_bias", "decoder.layers.0.self_attn.out_proj.weight", "decoder.layers.0.self_attn.out_proj.bias", "decoder.layers.0.multihead_attn.in_proj_weight", "decoder.layers.0.multihead_attn.in_proj_bias", "decoder.layers.0.multihead_attn.out_proj.weight", "decoder.layers.0.multihead_attn.out_proj.bias", "decoder.layers.0.linear1.weight", "decoder.layers.0.linear1.bias", "decoder.layers.0.linear2.weight", "decoder.layers.0.linear2.bias", "decoder.layers.0.norm1.weight", "decoder.layers.0.norm1.bias", "decoder.layers.0.norm2.weight", "decoder.layers.0.norm2.bias", "decoder.layers.0.norm3.weight", "decoder.layers.0.norm3.bias", "decoder.layers.1.self_attn.in_proj_weight", "decoder.layers.1.self_attn.in_proj_bias", "decoder.layers.1.self_attn.out_proj.weight", "decoder.layers.1.self_attn.out_proj.bias", "decoder.layers.1.multihead_attn.in_proj_weight", "decoder.layers.1.multihead_attn.in_proj_bias", "decoder.layers.1.multihead_attn.out_proj.weight", "decoder.layers.1.multihead_attn.out_proj.bias", "decoder.layers.1.linear1.weight", "decoder.layers.1.linear1.bias", "decoder.layers.1.linear2.weight", "decoder.layers.1.linear2.bias", "decoder.layers.1.norm1.weight", "decoder.layers.1.norm1.bias", "decoder.layers.1.norm2.weight", "decoder.layers.1.norm2.bias", "decoder.layers.1.norm3.weight", "decoder.layers.1.norm3.bias", "decoder.layers.2.self_attn.in_proj_weight", "decoder.layers.2.self_attn.in_proj_bias", "decoder.layers.2.self_attn.out_proj.weight", "decoder.layers.2.self_attn.out_proj.bias", "decoder.layers.2.multihead_attn.in_proj_weight", "decoder.layers.2.multihead_attn.in_proj_bias", "decoder.layers.2.multihead_attn.out_proj.weight", "decoder.layers.2.multihead_attn.out_proj.bias", "decoder.layers.2.linear1.weight", "decoder.layers.2.linear1.bias", "decoder.layers.2.linear2.weight", "decoder.layers.2.linear2.bias", "decoder.layers.2.norm1.weight", "decoder.layers.2.norm1.bias", "decoder.layers.2.norm2.weight", "decoder.layers.2.norm2.bias", "decoder.layers.2.norm3.weight", "decoder.layers.2.norm3.bias", "output_proj.weight", "output_proj.bias", "encoder_norm.weight", "encoder_norm.bias", "decoder_norm.weight", "decoder_norm.bias". 
	Unexpected key(s) in state_dict: "_orig_mod.pos_encoder", "_orig_mod.embed.weight", "_orig_mod.encoder.layers.0.self_attn.in_proj_weight", "_orig_mod.encoder.layers.0.self_attn.in_proj_bias", "_orig_mod.encoder.layers.0.self_attn.out_proj.weight", "_orig_mod.encoder.layers.0.self_attn.out_proj.bias", "_orig_mod.encoder.layers.0.linear1.weight", "_orig_mod.encoder.layers.0.linear1.bias", "_orig_mod.encoder.layers.0.linear2.weight", "_orig_mod.encoder.layers.0.linear2.bias", "_orig_mod.encoder.layers.0.norm1.weight", "_orig_mod.encoder.layers.0.norm1.bias", "_orig_mod.encoder.layers.0.norm2.weight", "_orig_mod.encoder.layers.0.norm2.bias", "_orig_mod.decoder.layers.0.self_attn.in_proj_weight", "_orig_mod.decoder.layers.0.self_attn.in_proj_bias", "_orig_mod.decoder.layers.0.self_attn.out_proj.weight", "_orig_mod.decoder.layers.0.self_attn.out_proj.bias", "_orig_mod.decoder.layers.0.multihead_attn.in_proj_weight", "_orig_mod.decoder.layers.0.multihead_attn.in_proj_bias", "_orig_mod.decoder.layers.0.multihead_attn.out_proj.weight", "_orig_mod.decoder.layers.0.multihead_attn.out_proj.bias", "_orig_mod.decoder.layers.0.linear1.weight", "_orig_mod.decoder.layers.0.linear1.bias", "_orig_mod.decoder.layers.0.linear2.weight", "_orig_mod.decoder.layers.0.linear2.bias", "_orig_mod.decoder.layers.0.norm1.weight", "_orig_mod.decoder.layers.0.norm1.bias", "_orig_mod.decoder.layers.0.norm2.weight", "_orig_mod.decoder.layers.0.norm2.bias", "_orig_mod.decoder.layers.0.norm3.weight", "_orig_mod.decoder.layers.0.norm3.bias", "_orig_mod.output_proj.weight", "_orig_mod.output_proj.bias", "_orig_mod.encoder_norm.weight", "_orig_mod.encoder_norm.bias", "_orig_mod.decoder_norm.weight", "_orig_mod.decoder_norm.bias". 
2025-03-28 00:40:06.865 | INFO     | test_checkpoint_loading:test_checkpoint_dimensions_extraction:184 - Recreating model with correct dimensions from checkpoint
2025-03-28 00:40:07.035 | INFO     | test_checkpoint_loading:test_checkpoint_dimensions_extraction:195 - Loading with correct dimensions succeeded
2025-03-28 00:40:07.043 | INFO     | test_checkpoint_loading:test_main_extract_model_dimensions:218 - Loading checkpoint from /home/h/Devel/latent/checkpoints/simpletransformer/latest.pt
2025-03-28 00:40:07.107 | INFO     | test_checkpoint_loading:test_main_extract_model_dimensions:225 - Extracted dimensions: {'d_model': 320, 'num_layers': 5, 'vocab_size': 12, 'num_latent': 10, 'max_len': 20}
2025-03-28 00:40:07.277 | INFO     | test_checkpoint_resume_unit:test_checkpoint_optimizer_persistence:172 - Starting first training run (15 steps)
2025-03-28 00:40:10.701 | INFO     | test_checkpoint_resume_unit:test_checkpoint_optimizer_persistence:193 - Looking for checkpoint at: /home/h/Devel/latent/checkpoints/simpletransformer/simpletransformer_latest.pt
2025-03-28 00:40:10.819 | INFO     | test_checkpoint_resume_unit:test_checkpoint_optimizer_persistence:212 - Last step from checkpoint - Simple: 10
2025-03-28 00:40:10.819 | INFO     | test_checkpoint_resume_unit:test_checkpoint_optimizer_persistence:213 - Last step from checkpoint - Latent: 10
2025-03-28 00:40:10.819 | INFO     | test_checkpoint_resume_unit:test_checkpoint_optimizer_persistence:214 - Simple optimizer info: {'lr': 1.4999999999999999e-05, 'weight_decay': 0.04, 'state_keys': 38, 'exp_avg_mean': 2.155610673071351e-05, 'exp_avg_sq_mean': 7.266904589187106e-08, 'step': 10}
2025-03-28 00:40:10.820 | INFO     | test_checkpoint_resume_unit:test_checkpoint_optimizer_persistence:215 - Simple scheduler info: {'last_epoch': 10, 'step_count': 11, 'base_lrs': [0.0003], 'verbose': False}
2025-03-28 00:40:10.820 | INFO     | test_checkpoint_resume_unit:test_checkpoint_optimizer_persistence:221 - Last LR from checkpoint - Simple: 1.4999999999999999e-05
2025-03-28 00:40:10.820 | INFO     | test_checkpoint_resume_unit:test_checkpoint_optimizer_persistence:222 - Last LR from checkpoint - Latent: 1.4999999999999999e-05
2025-03-28 00:40:10.820 | INFO     | test_checkpoint_resume_unit:test_checkpoint_optimizer_persistence:225 - Starting second training run (resumed, +15 steps)
2025-03-28 00:40:48.396 | INFO     | test_extract_dimensions:test_extract_dimensions_from_config:68 - Dimensions extracted correctly from config
2025-03-28 00:40:48.397 | INFO     | test_extract_dimensions:test_extract_dimensions_from_state_dict:88 - Dimensions extracted correctly from state dict
2025-03-28 00:40:48.399 | INFO     | test_extract_dimensions:test_extract_dimensions_with_direct_state_dict:107 - Dimensions extracted correctly from direct state dict
2025-03-28 00:40:48.419 | INFO     | test_extract_dimensions:test_extract_dimensions_with_real_checkpoint:126 - Dimensions extracted from real checkpoint: {'d_model': 320, 'num_layers': 5, 'vocab_size': 12, 'num_latent': 10, 'max_len': 20}
2025-03-28 00:40:48.419 | INFO     | test_extract_dimensions:test_extract_dimensions_with_real_checkpoint:134 - Dimensions extracted correctly from real checkpoint
2025-03-28 00:40:48.444 | INFO     | test_tensorboard_persistence:test_tensorboard_metrics_persistence:148 - Starting first training run (20 steps)
2025-03-28 00:41:01.290 | INFO     | test_train_from_scratch:test_training_from_scratch:35 - Running command: python main.py --d-model 64 --num-layers 2 --num-latent 4 --max-steps 5 --min-digits 1 --max-digits 1 --batch-size 16
2025-03-28 00:41:33.490 | INFO     | test_checkpoint_resume:test_checkpoint_resume_integration:152 - Starting initial training run
2025-03-28 00:41:33.490 | INFO     | test_checkpoint_resume:run_command_with_logging:104 - Running command: python main.py --d-model 32 --num-layers 1 --num-latent 2 --min-digits 1 --max-digits 1 --batch-size 8 --max-steps 10 --save-every 5 --seed 42
2025-03-28 00:41:37.740 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: /home/h/Devel/latent/src/TrainingLoop.py:356: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
2025-03-28 00:41:37.741 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: /home/h/Devel/latent/src/TrainingLoop.py:375: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
2025-03-28 00:41:40.510 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: Training (steps):   0%|          | 0/10 [00:00<?, ?it/s]/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:41:40.511 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:41:43.620 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:41:43.620 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:41:44.286 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: Training (steps):  10%|â–ˆ         | 1/10 [00:06<00:58,  6.54s/it, simple_loss=1.6329, latent_loss=1.5669]/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:41:44.286 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:41:44.638 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: Val SimpleTransformer:   0%|          | 0/8 [00:00<?, ?it/s][A/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)
2025-03-28 00:41:44.851 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:41:44.851 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:41:44.980 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: Val SimpleTransformer:  12%|â–ˆâ–Ž        | 1/8 [00:00<00:03,  2.12it/s][A/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:41:44.980 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:41:45.558 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:41:45.558 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:41:46.094 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: [A/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:41:46.094 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:41:46.579 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:41:46.579 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:41:46.940 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:41:46.941 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:41:47.150 | ERROR    | test_checkpoint_resume:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:41:47.150 | ERROR    | test_checkpoint_resume:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:41:47.150 | ERROR    | test_checkpoint_resume:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:41:47.151 | ERROR    | test_checkpoint_resume:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:41:47.151 | ERROR    | test_checkpoint_resume:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:41:47.846 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: Val LatentTransformer:   0%|          | 0/8 [00:00<?, ?it/s][A/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:41:47.846 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:41:47.964 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: Val LatentTransformer:  12%|â–ˆâ–Ž        | 1/8 [00:00<00:05,  1.24it/s][A/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:41:47.964 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:41:49.622 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:41:49.623 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:41:50.595 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: [A/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:41:50.595 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:41:51.478 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:41:51.478 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:41:51.669 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:41:51.669 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:41:55.403 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: Training (steps):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:14<00:04,  1.62s/it, simple_loss=1.8021, latent_loss=1.7534]/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:41:55.404 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:42:01.217 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:42:01.217 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:42:01.630 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: Training (steps):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:23<00:04,  1.62s/it, simple_loss=1.1025, latent_loss=1.1294]/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:42:01.631 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:42:01.723 | ERROR    | test_checkpoint_resume:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:42:01.724 | ERROR    | test_checkpoint_resume:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:42:01.724 | ERROR    | test_checkpoint_resume:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:42:01.724 | ERROR    | test_checkpoint_resume:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:42:02.040 | ERROR    | test_checkpoint_resume:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:42:02.040 | ERROR    | test_checkpoint_resume:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:42:02.041 | ERROR    | test_checkpoint_resume:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:42:02.041 | ERROR    | test_checkpoint_resume:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:42:02.041 | ERROR    | test_checkpoint_resume:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:42:02.377 | ERROR    | test_checkpoint_resume:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:42:02.377 | ERROR    | test_checkpoint_resume:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:42:02.377 | ERROR    | test_checkpoint_resume:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:42:02.378 | ERROR    | test_checkpoint_resume:_read_output:96 - STDOUT: Errors at positions: [0]
2025-03-28 00:42:04.699 | INFO     | test_checkpoint_resume:test_checkpoint_resume_integration:180 - Initial training process exit code: 0
2025-03-28 00:42:04.700 | INFO     | test_checkpoint_resume:test_checkpoint_resume_integration:187 - Initial training completed successfully
2025-03-28 00:42:04.700 | WARNING  | test_checkpoint_resume:get_latest_run_id_from_runs_file:35 - runs.json file not found
2025-03-28 00:42:04.700 | INFO     | test_checkpoint_resume:test_checkpoint_resume_integration:201 - Found run ID from directory listing: 20250328-004137_d32_l1_n2
2025-03-28 00:42:04.700 | INFO     | test_checkpoint_resume:test_checkpoint_resume_integration:204 - Extracted run ID: 20250328-004137_d32_l1_n2
2025-03-28 00:42:04.718 | INFO     | test_checkpoint_resume:test_checkpoint_resume_integration:217 - SimpleTransformer checkpoint created at step 10
2025-03-28 00:42:05.719 | INFO     | test_checkpoint_resume:test_checkpoint_resume_integration:223 - Resuming training for more steps with run ID: 20250328-004137_d32_l1_n2
2025-03-28 00:42:05.720 | INFO     | test_checkpoint_resume:run_command_with_logging:104 - Running command: python main.py --resume --run-id 20250328-004137_d32_l1_n2 --max-steps 20 --seed 42
2025-03-28 00:42:07.770 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: 2025-03-28 00:42:07.770 | INFO     | __main__:main:413 - Using checkpoint paths from run 20250328-004137_d32_l1_n2
2025-03-28 00:42:07.771 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: 2025-03-28 00:42:07.770 | INFO     | __main__:main:430 - Loading checkpoints to extract model dimensions
2025-03-28 00:42:08.175 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: 2025-03-28 00:42:08.175 | INFO     | __main__:main:439 - Dimensions from SimpleTransformer checkpoint: {'d_model': 320, 'num_layers': 5, 'vocab_size': 12, 'num_latent': 10, 'max_len': 20}
2025-03-28 00:42:08.176 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: 2025-03-28 00:42:08.175 | INFO     | __main__:main:440 - Dimensions from LatentTransformer checkpoint: {'d_model': 320, 'num_layers': 5, 'vocab_size': 12, 'num_latent': 10, 'max_len': 20}
2025-03-28 00:42:08.176 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: 2025-03-28 00:42:08.176 | INFO     | __main__:main:444 - Setting d_model to 320 from checkpoint
2025-03-28 00:42:08.176 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: 2025-03-28 00:42:08.176 | INFO     | __main__:main:448 - Setting num_layers to 5 from checkpoint
2025-03-28 00:42:08.176 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: 2025-03-28 00:42:08.176 | INFO     | __main__:main:452 - Setting num_latent to 10 from checkpoint
2025-03-28 00:42:08.177 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: 2025-03-28 00:42:08.176 | INFO     | __main__:main:459 - Using vocab_size=12 from checkpoint
2025-03-28 00:42:11.004 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: 2025-03-28 00:42:11.004 | INFO     | __main__:main:694 - Loading SimpleTransformer checkpoint from checkpoints/simpletransformer/simpletransformer_latest.pt
2025-03-28 00:42:11.053 | ERROR    | test_checkpoint_resume:_read_output:96 - STDERR: 2025-03-28 00:42:11.053 | ERROR    | __main__:main:702 - Error loading SimpleTransformer weights: Error(s) in loading state_dict for OptimizedModule:
2025-03-28 00:42:11.054 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.pos_encoder: copying a param with shape torch.Size([20, 32]) from checkpoint, the shape in current model is torch.Size([20, 320]).
2025-03-28 00:42:11.054 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.embed.weight: copying a param with shape torch.Size([12, 32]) from checkpoint, the shape in current model is torch.Size([12, 320]).
2025-03-28 00:42:11.054 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([96, 32]) from checkpoint, the shape in current model is torch.Size([960, 320]).
2025-03-28 00:42:11.054 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([960]).
2025-03-28 00:42:11.054 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([32, 32]) from checkpoint, the shape in current model is torch.Size([320, 320]).
2025-03-28 00:42:11.055 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.055 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.linear1.weight: copying a param with shape torch.Size([128, 32]) from checkpoint, the shape in current model is torch.Size([1280, 320]).
2025-03-28 00:42:11.055 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.linear1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1280]).
2025-03-28 00:42:11.055 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.linear2.weight: copying a param with shape torch.Size([32, 128]) from checkpoint, the shape in current model is torch.Size([320, 1280]).
2025-03-28 00:42:11.056 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.linear2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.056 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.norm1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.056 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.norm1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.056 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.norm2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.057 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.norm2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.057 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([96, 32]) from checkpoint, the shape in current model is torch.Size([960, 320]).
2025-03-28 00:42:11.057 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([960]).
2025-03-28 00:42:11.057 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([32, 32]) from checkpoint, the shape in current model is torch.Size([320, 320]).
2025-03-28 00:42:11.058 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.058 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.multihead_attn.in_proj_weight: copying a param with shape torch.Size([96, 32]) from checkpoint, the shape in current model is torch.Size([960, 320]).
2025-03-28 00:42:11.058 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.multihead_attn.in_proj_bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([960]).
2025-03-28 00:42:11.058 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.multihead_attn.out_proj.weight: copying a param with shape torch.Size([32, 32]) from checkpoint, the shape in current model is torch.Size([320, 320]).
2025-03-28 00:42:11.058 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.multihead_attn.out_proj.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.059 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.linear1.weight: copying a param with shape torch.Size([128, 32]) from checkpoint, the shape in current model is torch.Size([1280, 320]).
2025-03-28 00:42:11.059 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.linear1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1280]).
2025-03-28 00:42:11.059 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.linear2.weight: copying a param with shape torch.Size([32, 128]) from checkpoint, the shape in current model is torch.Size([320, 1280]).
2025-03-28 00:42:11.059 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.linear2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.059 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.norm1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.060 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.norm1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.060 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.norm2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.060 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.norm2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.060 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.norm3.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.060 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.norm3.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.061 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.output_proj.weight: copying a param with shape torch.Size([12, 32]) from checkpoint, the shape in current model is torch.Size([12, 320]).
2025-03-28 00:42:11.061 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder_norm.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.061 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder_norm.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.061 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder_norm.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.062 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder_norm.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.062 | ERROR    | test_checkpoint_resume:_read_output:96 - STDERR: 2025-03-28 00:42:11.053 | ERROR    | __main__:main:705 - This might be due to dimension mismatch. Check model configuration.
2025-03-28 00:42:11.062 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: 2025-03-28 00:42:11.054 | INFO     | __main__:main:744 - Loading LatentTransformer checkpoint from checkpoints/latenttransformer/latenttransformer_latest.pt
2025-03-28 00:42:11.121 | ERROR    | test_checkpoint_resume:_read_output:96 - STDERR: 2025-03-28 00:42:11.121 | ERROR    | __main__:main:752 - Error loading LatentTransformer weights: Error(s) in loading state_dict for OptimizedModule:
2025-03-28 00:42:11.122 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.pos_encoder: copying a param with shape torch.Size([20, 32]) from checkpoint, the shape in current model is torch.Size([20, 320]).
2025-03-28 00:42:11.122 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_tokens: copying a param with shape torch.Size([2, 32]) from checkpoint, the shape in current model is torch.Size([10, 320]).
2025-03-28 00:42:11.122 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.embed.weight: copying a param with shape torch.Size([12, 32]) from checkpoint, the shape in current model is torch.Size([12, 320]).
2025-03-28 00:42:11.123 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([96, 32]) from checkpoint, the shape in current model is torch.Size([960, 320]).
2025-03-28 00:42:11.123 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([960]).
2025-03-28 00:42:11.123 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([32, 32]) from checkpoint, the shape in current model is torch.Size([320, 320]).
2025-03-28 00:42:11.124 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.124 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.linear1.weight: copying a param with shape torch.Size([128, 32]) from checkpoint, the shape in current model is torch.Size([1280, 320]).
2025-03-28 00:42:11.124 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.linear1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1280]).
2025-03-28 00:42:11.125 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.linear2.weight: copying a param with shape torch.Size([32, 128]) from checkpoint, the shape in current model is torch.Size([320, 1280]).
2025-03-28 00:42:11.125 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.linear2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.125 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.norm1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.125 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.norm1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.126 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.norm2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.126 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder.layers.0.norm2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.126 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_proj_q.weight: copying a param with shape torch.Size([32, 32]) from checkpoint, the shape in current model is torch.Size([320, 320]).
2025-03-28 00:42:11.127 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_proj_q.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.127 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_proj_k.weight: copying a param with shape torch.Size([32, 32]) from checkpoint, the shape in current model is torch.Size([320, 320]).
2025-03-28 00:42:11.127 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_proj_k.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.127 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_proj_v.weight: copying a param with shape torch.Size([32, 32]) from checkpoint, the shape in current model is torch.Size([320, 320]).
2025-03-28 00:42:11.128 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_proj_v.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.128 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_norm1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.128 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_norm1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.129 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_norm2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.129 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_norm2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.129 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_mlp.0.weight: copying a param with shape torch.Size([128, 32]) from checkpoint, the shape in current model is torch.Size([1280, 320]).
2025-03-28 00:42:11.130 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_mlp.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1280]).
2025-03-28 00:42:11.130 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_mlp.2.weight: copying a param with shape torch.Size([32, 128]) from checkpoint, the shape in current model is torch.Size([320, 1280]).
2025-03-28 00:42:11.130 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.latent_mlp.2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.131 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([96, 32]) from checkpoint, the shape in current model is torch.Size([960, 320]).
2025-03-28 00:42:11.131 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([960]).
2025-03-28 00:42:11.131 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([32, 32]) from checkpoint, the shape in current model is torch.Size([320, 320]).
2025-03-28 00:42:11.132 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.132 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.multihead_attn.in_proj_weight: copying a param with shape torch.Size([96, 32]) from checkpoint, the shape in current model is torch.Size([960, 320]).
2025-03-28 00:42:11.132 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.multihead_attn.in_proj_bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([960]).
2025-03-28 00:42:11.133 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.multihead_attn.out_proj.weight: copying a param with shape torch.Size([32, 32]) from checkpoint, the shape in current model is torch.Size([320, 320]).
2025-03-28 00:42:11.133 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.multihead_attn.out_proj.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.133 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.linear1.weight: copying a param with shape torch.Size([128, 32]) from checkpoint, the shape in current model is torch.Size([1280, 320]).
2025-03-28 00:42:11.133 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.linear1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1280]).
2025-03-28 00:42:11.134 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.linear2.weight: copying a param with shape torch.Size([32, 128]) from checkpoint, the shape in current model is torch.Size([320, 1280]).
2025-03-28 00:42:11.134 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.linear2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.134 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.norm1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.134 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.norm1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.135 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.norm2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.135 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.norm2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.136 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.norm3.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.137 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder.layers.0.norm3.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.138 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.output_proj.weight: copying a param with shape torch.Size([12, 32]) from checkpoint, the shape in current model is torch.Size([12, 320]).
2025-03-28 00:42:11.138 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder_norm.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.138 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.encoder_norm.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.139 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder_norm.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.139 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: size mismatch for _orig_mod.decoder_norm.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([320]).
2025-03-28 00:42:11.140 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: /home/h/Devel/latent/src/TrainingLoop.py:356: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
2025-03-28 00:42:11.140 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: /home/h/Devel/latent/src/TrainingLoop.py:375: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
2025-03-28 00:42:11.140 | ERROR    | test_checkpoint_resume:_read_output:96 - STDERR: Error loading SimpleTransformer checkpoint: loaded state dict contains a parameter group that doesn't match the size of optimizer's group
2025-03-28 00:42:11.141 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: simple_optimizer.load_state_dict(simple_checkpoint['optimizer_state_dict'])
2025-03-28 00:42:11.141 | ERROR    | test_checkpoint_resume:_read_output:96 - STDERR: raise ValueError(
2025-03-28 00:42:11.141 | ERROR    | test_checkpoint_resume:_read_output:96 - STDERR: ValueError: loaded state dict contains a parameter group that doesn't match the size of optimizer's group
2025-03-28 00:42:11.142 | ERROR    | test_checkpoint_resume:_read_output:96 - STDERR: Error loading LatentTransformer checkpoint: loaded state dict contains a parameter group that doesn't match the size of optimizer's group
2025-03-28 00:42:11.142 | INFO     | test_checkpoint_resume:_read_output:100 - STDERR: latent_optimizer.load_state_dict(latent_checkpoint['optimizer_state_dict'])
2025-03-28 00:42:11.142 | ERROR    | test_checkpoint_resume:_read_output:96 - STDERR: raise ValueError(
2025-03-28 00:42:11.143 | ERROR    | test_checkpoint_resume:_read_output:96 - STDERR: ValueError: loaded state dict contains a parameter group that doesn't match the size of optimizer's group
2025-03-28 00:42:17.252 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: Training (steps):   0%|          | 0/10 [00:00<?, ?it/s]/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:42:17.252 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:42:17.366 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:42:17.367 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:42:24.198 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:42:24.199 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:42:25.098 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: Training (steps):  10%|â–ˆ         | 1/10 [00:13<02:05, 13.93s/it, simple_loss=2.7523, latent_loss=2.7372]/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:42:25.099 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:42:27.312 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: Final SimpleTransformer validation:   0%|          | 0/4 [00:00<?, ?it/s][A/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:42:27.313 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:42:27.459 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: Final SimpleTransformer validation:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:01,  1.92it/s][A/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:42:27.459 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:42:28.062 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:42:28.062 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:42:28.455 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: [A/home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)
2025-03-28 00:42:28.736 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:42:28.736 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:42:29.314 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: /home/h/.pyenv/versions/3.10.2/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
2025-03-28 00:42:29.314 | WARNING  | test_checkpoint_resume:_read_output:98 - STDERR: warnings.warn(
2025-03-28 00:42:29.417 | INFO     | test_checkpoint_resume:test_checkpoint_resume_integration:284 - Cleaning up test run: 20250328-004137_d32_l1_n2
2025-03-28 00:42:29.417 | INFO     | test_checkpoint_resume:cleanup_test_run:67 - Removing run directory: runs/parallel_comparison/20250328-004137_d32_l1_n2
2025-03-28 00:42:29.418 | INFO     | test_checkpoint_resume:test_checkpoint_resume_integration:290 - Removing log file: initial_run.log
2025-03-28 00:42:29.418 | INFO     | test_checkpoint_resume:test_checkpoint_resume_integration:290 - Removing log file: resume_run.log
2025-03-28 00:43:28.454 | INFO     | test_checkpoint_resume:test_checkpoint_resume_integration:68 - Creating mock checkpoint for SimpleTransformer
2025-03-28 00:43:28.455 | INFO     | test_checkpoint_resume:test_checkpoint_resume_integration:78 - Creating mock checkpoint for LatentTransformer
2025-03-28 00:43:28.455 | INFO     | test_checkpoint_resume:test_checkpoint_resume_integration:91 - SimpleTransformer dimensions: {'d_model': 64, 'num_layers': 2, 'vocab_size': 12, 'num_latent': 0, 'max_len': 20}
2025-03-28 00:43:28.455 | INFO     | test_checkpoint_resume:test_checkpoint_resume_integration:92 - LatentTransformer dimensions: {'d_model': 64, 'num_layers': 2, 'vocab_size': 12, 'num_latent': 4, 'max_len': 20}
2025-03-28 00:43:28.456 | INFO     | test_checkpoint_resume:test_checkpoint_resume_integration:95 - Creating models with wrong dimensions
2025-03-28 00:43:28.467 | INFO     | test_checkpoint_resume:test_checkpoint_resume_integration:125 - Recreating models with correct dimensions
2025-03-28 00:43:28.483 | INFO     | test_checkpoint_resume:test_checkpoint_resume_integration:149 - Testing state_dict loading for SimpleTransformer
2025-03-28 00:43:28.484 | INFO     | test_checkpoint_resume:test_checkpoint_resume_integration:153 - Testing state_dict loading for LatentTransformer
2025-03-28 00:43:28.485 | INFO     | test_checkpoint_resume:test_checkpoint_resume_integration:157 - Testing positional encoder dimension handling
2025-03-28 00:43:28.494 | INFO     | test_checkpoint_resume:test_checkpoint_resume_integration:191 - Integration test completed successfully
